# üé• ReadBuddy 2.0 - AI-Powered Multi-Segment Video Analyzer

[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/fafawds67685da/Readbuddy_2.0)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Chrome](https://img.shields.io/badge/chrome-extension-red.svg)](https://chrome.google.com/webstore)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-009688.svg)](https://fastapi.tiangolo.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-EE4C2C.svg)](https://pytorch.org/)

**ReadBuddy 2.0** is an advanced Chrome extension that automatically breaks down YouTube videos into 30-second segments, captures 6 frames per segment at 5-second intervals, generates AI captions for each frame, summarizes the content, and narrates it via text-to-speech - creating a complete audio description timeline for visual content.
---

## üìã Table of Contents

- [Key Features](#-key-features)
- [Project Architecture](#-project-architecture)
- [File Directory Structure](#-file-directory-structure)
- [Technology Stack](#-technology-stack)
- [System Pipeline](#-system-pipeline)
- [Installation Guide](#-installation-guide)
- [How to Run](#-how-to-run)
- [How It Works](#-how-it-works)
- [Use Cases](#-use-cases)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Performance](#-performance)
- [Future Scope](#-future-scope)
- [Contributing](#-contributing)
- [License](#-license)

---

## üåü Key Features

### üé¨ **Segmented Video Analysis**
- **Automatic Segmentation**: Breaks videos into 30-second chunks
- **Frame Capture**: Captures 6 frames per segment (every 5 seconds)
- **Perfect Timing**: Frames at 5s, 10s, 15s, 20s, 25s, 30s marks
- **Smart Pause/Resume**: Pauses after each segment for TTS narration
- **Multi-Segment Support**: Handles videos of any length (e.g., 261s = 9 segments)

### ü§ñ **AI-Powered Analysis**
- **BLIP-Base Image Captioning**: Generates natural language descriptions
- **BART-Large-CNN Summarization**: Summarizes frame captions into coherent narratives
- **T5-Base Video Summarization**: Alternative summarization model
- **Real-time Processing**: Processes each segment independently

### üîä **Text-to-Speech Integration**
- **Web Speech API**: Native browser TTS
- **Automatic Narration**: Speaks summary after each segment
- **Auto-Resume**: Resumes video playback after TTS completes
- **Multi-language Support**: Supports all Chrome TTS voices

### ‚ö° **Advanced Error Handling**
- **15-Second Grace Period**: Waits for slow backend responses
- **Retry Logic**: 3-attempt retry for frame captures
- **Fallback Mechanisms**: Multiple capture methods
- **Recovery System**: Auto-recovers from errors and continues

### üìä **Real-time Monitoring**
- **Live Countdown**: Shows remaining time in current segment
- **Progress Tracking**: Displays segment X/Y progress
- **Frame Counter**: Shows frames captured/expected
- **Caption Status**: Real-time caption reception updates

---

## üèóÔ∏è Project Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CHROME BROWSER                              ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   YouTube    ‚îÇ         ‚îÇ Content.js   ‚îÇ      ‚îÇ Sidepanel.js  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Video      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ              ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ         ‚îÇ ‚Ä¢ Finds video‚îÇ      ‚îÇ ‚Ä¢ UI Display  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ [==========] ‚îÇ         ‚îÇ ‚Ä¢ Segments   ‚îÇ      ‚îÇ ‚Ä¢ TTS Control ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚ñ∂Ô∏è 30s/261s ‚îÇ         ‚îÇ ‚Ä¢ Captures   ‚îÇ      ‚îÇ ‚Ä¢ User Input  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ         ‚îÇ ‚Ä¢ Pauses     ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                                   ‚îÇ                                ‚îÇ
‚îÇ                                   ‚ñº                                ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ                          ‚îÇBackground.js ‚îÇ                          ‚îÇ
‚îÇ                          ‚îÇ              ‚îÇ                          ‚îÇ
‚îÇ                          ‚îÇ ‚Ä¢ Routes msg ‚îÇ                          ‚îÇ
‚îÇ                          ‚îÇ ‚Ä¢ Forwards   ‚îÇ                          ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ HTTP POST
                                  ‚îÇ Base64 Image
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASTAPI BACKEND (Port 8000)                      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                      main.py                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Endpoints:                                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /caption-image      ‚Üí BLIP-base captioning           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /summarize-captions ‚Üí BART/T5 summarization          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /                   ‚Üí Health check                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Models Loaded:                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  üì¶ BLIP-base (Salesforce/blip-image-captioning-base)        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  üì¶ BART-large-CNN (facebook/bart-large-cnn)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  üì¶ T5-base (t5-base)                                         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Processing Pipeline:                                               ‚îÇ
‚îÇ  1. Receive Base64 image                                           ‚îÇ
‚îÇ  2. Decode ‚Üí PIL Image                                             ‚îÇ
‚îÇ  3. Preprocess (resize, normalize)                                 ‚îÇ
‚îÇ  4. BLIP Model ‚Üí Generate caption                                  ‚îÇ
‚îÇ  5. Return JSON: {"caption": "A person sitting..."}               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Summarization Pipeline:                                            ‚îÇ
‚îÇ  1. Receive array of captions                                      ‚îÇ
‚îÇ  2. Concatenate with timestamps                                    ‚îÇ
‚îÇ  3. BART/T5 ‚Üí Generate summary                                     ‚îÇ
‚îÇ  4. Return JSON: {"summary": "Cartoon shows..."}                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ File Directory Structure

```
Readbuddy_2.0/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Chrome Extension/           # Frontend - Chrome Extension
‚îÇ   ‚îú‚îÄ‚îÄ background.js              # Service worker - message routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Keyboard command listener (Alt+1/2/3/4)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Debounce mechanism (300ms cooldown)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Content script auto-injection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Backend proxy (CORS workaround)
‚îÇ   ‚îú‚îÄ‚îÄ content.js                 # Video analysis logic (1,265 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Segmented analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Frame capture (3 methods)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Grace period handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Pause/resume control
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Keyboard command handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TTS initialization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Error recovery
‚îÇ   ‚îú‚îÄ‚îÄ manifest.json              # Extension configuration (Manifest V3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 4 keyboard shortcuts defined
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Permissions (activeTab, scripting, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Background service worker
‚îÇ   ‚îú‚îÄ‚îÄ sidepanel.html             # User interface HTML
‚îÇ   ‚îî‚îÄ‚îÄ sidepanel.js               # UI logic & TTS control (777 lines)
‚îÇ       ‚îú‚îÄ‚îÄ Button handlers
‚îÇ       ‚îú‚îÄ‚îÄ Message listeners
‚îÇ       ‚îú‚îÄ‚îÄ TTS with callbacks
‚îÇ       ‚îî‚îÄ‚îÄ Resume command sender
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Backend/                    # Backend - FastAPI Server
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # API server (all endpoints & AI models)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ /caption-image         # BLIP captioning endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ /summarize-captions    # BART/T5 summarization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Model loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Image preprocessing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Error handling
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/               # Python cache (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ env/                        # Python virtual environment
‚îÇ   ‚îú‚îÄ‚îÄ Lib/                       # Installed packages
‚îÇ   ‚îú‚îÄ‚îÄ Scripts/                   # Executables (python.exe, pip.exe)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uvicorn.exe            # ASGI server
‚îÇ   ‚îî‚îÄ‚îÄ pyvenv.cfg                 # Environment config
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                     # Git ignore rules
‚îú‚îÄ‚îÄ Future scope.txt               # Feature roadmap
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
    ‚îú‚îÄ‚îÄ fastapi==0.104.1
    ‚îú‚îÄ‚îÄ uvicorn[standard]==0.24.0
    ‚îú‚îÄ‚îÄ torch==2.1.0+cpu
    ‚îú‚îÄ‚îÄ torchvision==0.16.0+cpu
    ‚îú‚îÄ‚îÄ transformers==4.35.0
    ‚îú‚îÄ‚îÄ Pillow==10.1.0
    ‚îî‚îÄ‚îÄ python-multipart==0.0.6
```

### üì¶ Key Files Explained

| File | Lines | Purpose |
|------|-------|---------|
| `content.js` | 1,265 | Core video analysis engine |
| `sidepanel.js` | 777 | UI and TTS coordination |
| `background.js` | ~500 | Message routing between tabs |
| `main.py` | ~400 | All backend logic & AI models |
| `manifest.json` | ~50 | Extension permissions & config |

---

## üõ†Ô∏è Technology Stack

### Backend Technologies

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.9+ | Core programming language |
| **FastAPI** | 0.104.1 | Modern async web framework |
| **Uvicorn** | 0.24.0 | ASGI server (async support) |
| **PyTorch** | 2.1.0 | Deep learning framework |
| **Transformers** | 4.35.0 | HuggingFace model library |
| **Pillow** | 10.1.0 | Image processing |

### AI Models

| Model | Size | Purpose | Accuracy |
|-------|------|---------|----------|
| **BLIP-base** | ~990MB | Image captioning | ~85% |
| **BART-large-CNN** | ~1.6GB | Text summarization | ~92% |
| **T5-base** | ~850MB | Alternative summarization | ~88% |

**Total Model Size:** ~3.5GB (downloaded on first run)

### Frontend Technologies

| Technology | Purpose |
|------------|---------|
| **Chrome Extensions API** | Manifest V3 framework |
| **Web Speech API** | Text-to-speech (native) |
| **Canvas API** | Frame capture method |
| **Vanilla JavaScript** | No external dependencies |
| **HTML5/CSS3** | User interface |

---

## üîÑ System Pipeline

### Complete Request-Response Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SEGMENT ANALYSIS CYCLE (Repeats every 30 seconds)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: USER INITIATES
  User clicks "Multi-Frame Video Analysis"
    ‚Üì
  content.js: startVideoAnalysisLoop()
    ‚Üì
  Find video element: document.querySelector('video')
    ‚Üì
  Calculate segments: Math.ceil(videoDuration / 30)
    ‚Üì
  Start analysis interval: setInterval(..., 1000ms)

Step 2: FRAME CAPTURE (Every 5 seconds within 30s segment)
  Countdown: 5s
    ‚Üì
  content.js: captureVideoFrame()
    ‚Üì
  Try Method 1: Canvas API
    ‚îú‚îÄ Success ‚Üí canvas.toDataURL('image/jpeg', 0.8)
    ‚îî‚îÄ CORS Error ‚Üí Try Method 2
    
  Try Method 2: MediaStream API
    ‚îú‚îÄ Success ‚Üí video.captureStream() ‚Üí grabFrame()
    ‚îî‚îÄ Failed ‚Üí Try Method 3
    
  Try Method 3: Screenshot API
    ‚îú‚îÄ chrome.tabs.captureVisibleTab()
    ‚îú‚îÄ Crop to video rectangle
    ‚îî‚îÄ Always succeeds ‚úì
    
  Result: Base64 JPEG string (~200-500KB)

Step 3: CAPTION GENERATION
  content.js ‚Üí background.js (chrome.runtime.sendMessage)
    ‚Üì
  background.js ‚Üí Backend (fetch POST)
    ‚Üì
  POST http://127.0.0.1:8000/caption-image
    Body: { "image_data": "data:image/jpeg;base64,..." }
    ‚Üì
  Backend: main.py
    ‚îú‚îÄ Decode Base64 ‚Üí bytes
    ‚îú‚îÄ bytes ‚Üí PIL.Image
    ‚îú‚îÄ Preprocess: RGB, resize, normalize
    ‚îú‚îÄ BLIP processor(images=[img])
    ‚îú‚îÄ model.generate(**inputs)
    ‚îî‚îÄ Return caption
    
  Response: {
    "caption": "A cartoon of a man riding on top of a horse...",
    "status": "success"
  }
  Processing time: ~2-5s (CPU), ~0.5-1s (GPU)

Step 4: COLLECT CAPTIONS (Repeat 6 times: 5s, 10s, 15s, 20s, 25s, 30s)
  content.js: captionBuffer = []
    ‚Üì
  Frame 1 (5s) ‚Üí captionBuffer.push(caption1)
  Frame 2 (10s) ‚Üí captionBuffer.push(caption2)
  Frame 3 (15s) ‚Üí captionBuffer.push(caption3)
  Frame 4 (20s) ‚Üí captionBuffer.push(caption4)
  Frame 5 (25s) ‚Üí captionBuffer.push(caption5)
  Frame 6 (30s) ‚Üí captionBuffer.push(caption6)
    ‚Üì
  30s countdown reached ‚Üí PAUSE VIDEO
    ‚Üì
  pauseVideoForNarration()
    ‚îú‚îÄ video.pause()
    ‚îú‚îÄ isVideoPaused = true
    ‚îú‚îÄ windowEndTimestamp = Date.now()
    ‚îî‚îÄ Wait for all captions...

Step 5: GRACE PERIOD (Wait up to 15 seconds for late captions)
  Grace Check Loop (every 1 second):
    ‚Üì
  Check: (receivedFrameCount === expectedFrameCount) 
         OR 
         (Date.now() - windowEndTimestamp > 15000ms)
    ‚Üì
  If TRUE ‚Üí Trigger summarization
  If FALSE ‚Üí Keep waiting...
    
  Example logs:
    ‚è≥ Grace check: waited=1.0s/15s, received=0/6
    ‚è≥ Grace check: waited=2.0s/15s, received=2/6
    ‚è≥ Grace check: waited=3.0s/15s, received=4/6
    ‚è≥ Grace check: waited=4.0s/15s, received=6/6 ‚úì ALL RECEIVED!

Step 6: SUMMARIZATION
  content.js: sendCaptionsForSummarization(captionBuffer)
    ‚Üì
  POST http://127.0.0.1:8000/summarize-captions
    Body: {
      "captions": [
        "A cartoon of a man riding on top of a horse...",
        "A man with long hair and brown eyes looks at the camera...",
        "A scene from the animated movie alaa...",
        ...
      ]
    }
    ‚Üì
  Backend: main.py
    ‚îú‚îÄ Join captions: "Frame 1: ... Frame 2: ..."
    ‚îú‚îÄ BART tokenizer.encode(input_text)
    ‚îú‚îÄ model.generate(max_length=400, num_beams=4)
    ‚îú‚îÄ tokenizer.decode(summary_ids)
    ‚îî‚îÄ Return summary
    
  Response: {
    "summary": "Cartoon shows man riding on top of a horse in front of desert. sun sets behind him as he watches the sun set behind him. image is part of 30-second video sequence filmed in texas and florida.",
    "success": true
  }
  Processing time: ~3-8s (CPU), ~1-2s (GPU)

Step 7: TEXT-TO-SPEECH
  content.js ‚Üí sidepanel.js (chrome.runtime.sendMessage)
    Action: 'videoSequenceAnalyzed'
    Data: { summary, captions, frameCount }
    ‚Üì
  sidepanel.js: speakWithCallback()
    ‚Üì
  Create utterance:
    const utterance = new SpeechSynthesisUtterance(summary);
    utterance.rate = 1.0; // User configurable
    utterance.lang = 'en-US';
    ‚Üì
  Speak:
    window.speechSynthesis.speak(utterance);
    
  Listen for completion:
    utterance.onend = () => {
      console.log('‚úÖ TTS completed');
      sendResumeVideoCommand(); // Resume video!
    };
    
  TTS duration: ~10-20 seconds (depends on summary length)

Step 8: RESUME VIDEO ‚Üí NEXT SEGMENT
  sidepanel.js: sendResumeVideoCommand()
    ‚Üì
  chrome.tabs.sendMessage({ action: 'resumeVideo' })
    ‚Üì
  content.js: resumeVideoAfterNarration()
    ‚Üì
  Check: currentSegment < totalSegments (e.g., 1 < 9)
    ‚îú‚îÄ TRUE ‚Üí Move to next segment
    ‚îÇ   ‚îú‚îÄ currentSegment++ (1 ‚Üí 2)
    ‚îÇ   ‚îú‚îÄ Reset: captionBuffer = [], capturedFrameCount = 0
    ‚îÇ   ‚îú‚îÄ Reset: windowEndTimestamp = 0, summaryTriggered = false
    ‚îÇ   ‚îú‚îÄ video.play() ‚Üí Resume playback
    ‚îÇ   ‚îî‚îÄ Loop back to Step 2 (capture frames 31s-60s)
    ‚îÇ
    ‚îî‚îÄ FALSE ‚Üí All segments complete!
        ‚îî‚îÄ Display: "‚úÖ All 9 segments analyzed. Video complete!"

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TOTAL TIME PER SEGMENT (Example: 30s segment, 6 frames)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Frame Capture:     30s (video playing)                           ‚îÇ
‚îÇ ‚Ä¢ Caption Processing: 15-30s (6 frames √ó 2.5-5s each)             ‚îÇ
‚îÇ ‚Ä¢ Grace Period:      0-15s (waiting for slow captions)            ‚îÇ
‚îÇ ‚Ä¢ Summarization:     3-8s (BART processing)                        ‚îÇ
‚îÇ ‚Ä¢ TTS Narration:     10-20s (speaking summary)                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ TOTAL: ~60-105 seconds per 30s segment                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì• Installation Guide

### Prerequisites

‚úÖ **Required:**
- Windows 10/11, macOS 10.15+, or Linux
- Python 3.9 or higher
- Google Chrome or Chromium-based browser (Edge, Brave, Opera)
- 8GB RAM minimum (16GB recommended)
- 5GB free disk space (for AI models)
- Internet connection (for first-time model download)

‚úÖ **Optional:**
- NVIDIA GPU with CUDA support (10x faster processing)
- 4GB+ VRAM for GPU acceleration

### Step 1: Clone Repository

```bash
git clone https://github.com/fafawds67685da/Readbuddy_2.0.git
cd Readbuddy_2.0
```

### Step 2: Backend Setup

#### Create Virtual Environment

**Windows:**
```powershell
# Create virtual environment
python -m venv env

# Activate
env\Scripts\activate

# Verify activation (prompt should show "(env)")
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv env

# Activate
source env/bin/activate

# Verify activation
```

#### Install Dependencies

```bash
# Upgrade pip first
python -m pip install --upgrade pip

# Install all requirements
pip install -r requirements.txt
```

**Installation Progress:**
```
Collecting fastapi==0.104.1
  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)
Collecting torch==2.1.0
  Downloading torch-2.1.0-cp39-cp39-win_amd64.whl (197.9 MB)
  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 197.9/197.9 MB 15.2 MB/s
Collecting transformers==4.35.0
  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)
...
Successfully installed fastapi-0.104.1 torch-2.1.0 transformers-4.35.0 ...
```

**Estimated time:** 10-20 minutes (depending on internet speed)

#### First-Time Model Download

```bash
cd Backend
python main.py
```

**Console Output:**
```
‚è≥ Loading AI models (this may take a minute)...
üîß Using device: cpu

Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 4.52k/4.52k [00:00<00:00]
Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà| 990M/990M [02:15<00:00, 7.31MB/s]
Downloading (‚Ä¶)rocessor_config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 342/342 [00:00<00:00]
Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 695/695 [00:00<00:00]
Downloading (‚Ä¶)olve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00]

‚úÖ Image captioning model loaded: Salesforce/blip-image-captioning-base

Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 1.58k/1.58k [00:00<00:00]
Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà| 1.63G/1.63G [04:45<00:00, 5.71MB/s]
Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 26.0/26.0 [00:00<00:00]
Downloading (‚Ä¶)olve/main/vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00]

‚úÖ Summarization model loaded: facebook/bart-large-cnn

Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà| 1.44k/1.44k [00:00<00:00]
Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà| 892M/892M [02:05<00:00, 7.11MB/s]

‚úÖ Video summarization model loaded: t5-base

‚úÖ All models loaded successfully on cpu!

INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

**Estimated time:** 10-15 minutes (downloads ~3.5GB of models)

**Models are cached** in `~/.cache/huggingface/` and won't be downloaded again.

### Step 3: Chrome Extension Setup

#### Load Unpacked Extension

1. **Open Chrome Extensions Page:**
   - Navigate to `chrome://extensions/`
   - Or: Menu (‚ãÆ) ‚Üí More Tools ‚Üí Extensions

2. **Enable Developer Mode:**
   - Toggle switch in top-right corner

3. **Load Extension:**
   - Click **"Load unpacked"** button
   - Navigate to `Readbuddy_2.0/Chrome Extension/` folder
   - Click **"Select Folder"**

4. **Verify Installation:**
   - Extension appears in list: "ReadBuddy 2.0"
   - Status: Enabled ‚úì
   - ID: `chrome-extension://[random-id]/`

5. **Pin to Toolbar (Optional):**
   - Click puzzle icon üß© in Chrome toolbar
   - Find "ReadBuddy 2.0"
   - Click pin üìå icon

---

## üöÄ How to Run

### Starting the Backend Server

**Every time you want to use ReadBuddy:**

1. **Open Terminal/PowerShell**

2. **Navigate to Backend folder:**
   ```bash
   cd Readbuddy_2.0/Backend
   ```

3. **Activate virtual environment:**
   
   **Windows:**
   ```powershell
   ..\env\Scripts\activate
   ```
   
   **macOS/Linux:**
   ```bash
   source ../env/bin/activate
   ```

4. **Start server:**
   ```bash
   python main.py
   ```
   
   **Alternative (using uvicorn directly):**
   ```bash
   uvicorn main:app --reload --host 127.0.0.1 --port 8000
   ```

5. **Verify server is running:**
   - Open browser: http://127.0.0.1:8000
   - Should see: `{"status":"online","models_loaded":true,"device":"cpu"}`

**Console should show:**
```
‚úÖ All models loaded successfully on cpu!
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [12345]
INFO:     Started server process [67890]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

### Using the Extension

1. **Ensure backend is running** (see above)

2. **Open YouTube** or any page with `<video>` element:
   - YouTube: https://www.youtube.com/watch?v=YOUR-VIDEO-ID
   - Any HTML5 video player

3. **Click ReadBuddy extension icon** in toolbar
   - Side panel opens on right side

4. **Click "Multi-Frame Video Analysis"** button
   - Button turns RED: "STOP Video Analysis"
   - Extension starts analyzing

5. **Watch the magic happen:**
   ```
   üîÑ Video Analysis Started
   üìè Video: 261s total, 9 segments
   üìç Segment 1/9: 30s window, 6 frames
   
   üì∏ Capturing frame 1/6 at 5s...
   ‚úÖ Caption 1: "A cartoon of a man riding..."
   
   üì∏ Capturing frame 2/6 at 10s...
   ‚úÖ Caption 2: "A man with long hair..."
   
   ... (continues for 6 frames) ...
   
   ‚è∏Ô∏è Video paused for TTS narration
   üîä Speaking: "Cartoon shows man riding on top of horse..."
   
   ‚ñ∂Ô∏è Resuming video, moving to Segment 2/9...
   ```

6. **Stop analysis anytime:**
   - Click "STOP Video Analysis" button (turns GREEN again)

---

## üí° How It Works

### The Complete Workflow (Example: 261-second Video)

#### **Initialization Phase**

1. User clicks "Multi-Frame Video Analysis"
2. Extension finds video element: `document.querySelector('video')`
3. Reads video duration: `video.duration = 261 seconds`
4. Calculates segments: `Math.ceil(261 / 30) = 9 segments`
5. Sets up segment 1:
   - Duration: 30 seconds
   - Expected frames: `Math.floor(30 / 5) = 6 frames`
   - Timestamps: [5s, 10s, 15s, 20s, 25s, 30s]

#### **Segment 1 Analysis (0-30 seconds)**

**Frame Capture Loop:**
```
Countdown: 0s ‚Üí Video plays normally

Countdown: 5s ‚Üí CAPTURE FRAME 1
  ‚îú‚îÄ Pause video momentarily
  ‚îú‚îÄ Capture frame via Canvas API
  ‚îú‚îÄ Convert to Base64 JPEG
  ‚îú‚îÄ Send to backend: POST /caption-image
  ‚îú‚îÄ Backend processes: ~2-5 seconds
  ‚îî‚îÄ Resume video

Countdown: 10s ‚Üí CAPTURE FRAME 2
  ‚îî‚îÄ (repeat process)

Countdown: 15s ‚Üí CAPTURE FRAME 3
Countdown: 20s ‚Üí CAPTURE FRAME 4
Countdown: 25s ‚Üí CAPTURE FRAME 5
Countdown: 30s ‚Üí CAPTURE FRAME 6
```

**After 30 seconds:**
1. Video **PAUSES** (via `video.pause()`)
2. `windowEndTimestamp = Date.now()` (record pause time)
3. Start **Grace Period Loop** (check every 1 second)

**Grace Period (0-15 seconds):**
```
Check 1 (1s): Received 0/6 captions ‚Üí Keep waiting
Check 2 (2s): Received 2/6 captions ‚Üí Keep waiting
Check 3 (3s): Received 4/6 captions ‚Üí Keep waiting
Check 4 (4s): Received 6/6 captions ‚Üí ALL RECEIVED! ‚úì
  ‚Üí Trigger summarization
```

**Summarization:**
1. Collect all 6 captions from `captionBuffer`
2. Send to backend: `POST /summarize-captions`
   ```json
   {
     "captions": [
       "A cartoon of a man riding on top of a horse...",
       "A man with long hair and brown eyes...",
       "A scene from the animated movie...",
       "A scene from disney's alaa...",
       "A man standing in front of a building...",
       "A banner with the words of maha maha..."
     ]
   }
   ```
3. Backend processes with BART:
   - Tokenize input (6 captions joined)
   - Generate summary (max 400 tokens)
   - Return coherent narrative

4. Response:
   ```json
   {
     "summary": "Cartoon shows man riding on top of a horse in front of desert. sun sets behind him as he watches the sun set behind him. image is part of 30-second video sequence filmed in texas and florida.",
     "success": true
   }
   ```

**Text-to-Speech:**
1. Sidepanel receives summary
2. Creates speech utterance:
   ```javascript
   const utterance = new SpeechSynthesisUtterance(summary);
   utterance.rate = 1.0; // Normal speed
   utterance.lang = 'en-US';
   ```
3. Speaks summary (takes ~10-20 seconds)
4. On completion (`utterance.onend`):
   - Send resume command to content.js

**Resume & Next Segment:**
1. Content.js receives `resumeVideo` message
2. Checks: `currentSegment < totalSegments` (1 < 9 = true)
3. Increments: `currentSegment = 2`
4. Resets state:
   - `captionBuffer = []`
   - `capturedFrameCount = 0`
   - `receivedFrameCount = 0`
   - `summaryTriggered = false`
   - `windowEndTimestamp = 0`
5. Resumes video: `video.play()`
6. **Loop continues** for segment 2 (30s-60s)

#### **Segments 2-8** (same process)

Each segment follows identical flow:
- Capture 6 frames at 5s intervals
- Pause at end of segment
- Generate captions
- Summarize
- Speak via TTS
- Resume for next segment

#### **Segment 9 (Final)** (240-261 seconds)

Special handling for partial segment:
- Duration: `261 - 240 = 21 seconds`
- Expected frames: `Math.floor(21 / 5) = 4 frames`
- Timestamps: [245s, 250s, 255s, 260s]
- After completion:
  - `currentSegment = 9, totalSegments = 9`
  - Check: `9 < 9 = false`
  - **Analysis complete!**
  - Display: "‚úÖ All 9 segments analyzed"

---

## üéØ Use Cases

### 1. **Accessibility for Visually Impaired Users**
- **Problem:** YouTube videos often lack audio descriptions
- **Solution:** ReadBuddy generates real-time visual descriptions
- **Benefit:** Understand visual content through audio narration

### 2. **Educational Content Analysis**
- **Problem:** Long lecture videos are hard to review
- **Solution:** Get timestamped summaries of visual elements
- **Benefit:** Quickly identify key moments without watching entire video

### 3. **Content Moderation**
- **Problem:** Manual video review is time-consuming
- **Solution:** Automated frame analysis identifies visual content
- **Benefit:** Flag inappropriate content efficiently

### 4. **Video Summarization for Research**
- **Problem:** Researchers need to analyze hours of video footage
- **Solution:** Generate timestamped captions and summaries
- **Benefit:** Search and reference specific moments

### 5. **Language Learning**
- **Problem:** Understanding visual context in foreign language videos
- **Solution:** Get English descriptions of what's happening visually
- **Benefit:** Better comprehension without understanding audio

### 6. **Silent Video Environments**
- **Problem:** Watching videos in public/work without sound
- **Solution:** Read visual descriptions as text summaries
- **Benefit:** Understand content without audio

---

## ‚öôÔ∏è Configuration

### Customize Analysis Settings

#### **Change Capture Interval** (default: 5 seconds)

Edit `Chrome Extension/content.js` (line 14):
```javascript
// Current: Capture every 5 seconds
let captureIntervalSeconds = 5;

// Options:
let captureIntervalSeconds = 3;  // More frames, slower (10 frames/30s)
let captureIntervalSeconds = 10; // Fewer frames, faster (3 frames/30s)
```

#### **Change Segment Duration** (default: 30 seconds)

Edit `Chrome Extension/content.js` (line 15):
```javascript
// Current: 30-second segments
let analysisWindowSeconds = 30;

// Options:
let analysisWindowSeconds = 60;  // 1-minute segments
let analysisWindowSeconds = 15;  // 15-second segments
```

#### **Change Grace Period** (default: 15 seconds)

Edit `Chrome Extension/content.js` (line 33):
```javascript
// Current: Wait 15s for slow backend
const postWindowGraceSeconds = 15;

// Options:
const postWindowGraceSeconds = 20;  // Slower backend
const postWindowGraceSeconds = 10;  // Faster backend/GPU
```

#### **Change TTS Speed**

In sidepanel.html, adjust voice rate slider (0.5x - 2.0x).

### Backend Configuration

#### **Use GPU Instead of CPU**

If you have NVIDIA GPU with CUDA:

```bash
# Check CUDA availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# If True, install CUDA version of PyTorch
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Restart backend
python main.py
```

Expected output:
```
üîß Using device: cuda
‚úÖ All models loaded successfully on cuda!
```

**Performance improvement:**
- CPU: ~2-5 seconds per frame
- GPU: ~0.5-1 second per frame (3-5x faster!)

#### **Change Backend Port** (default: 8000)

Edit `Backend/main.py` (last line):
```python
# Current:
uvicorn.run(app, host="127.0.0.1", port=8000)

# Change to 8080:
uvicorn.run(app, host="127.0.0.1", port=8080)
```

Then update extension files:
```javascript
// Chrome Extension/background.js
// Chrome Extension/sidepanel.js
const API_URL = "http://127.0.0.1:8080"; // Changed from 8000
```

---

## üêõ Troubleshooting

### Common Issues & Solutions

#### **Issue 1: Backend won't start**

**Error:**
```
ModuleNotFoundError: No module named 'fastapi'
```

**Solution:**
```bash
# Ensure virtual environment is activated
# You should see (env) in prompt

# Windows:
env\Scripts\activate

# macOS/Linux:
source env/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

---

#### **Issue 2: "Port 8000 already in use"**

**Error:**
```
ERROR: [Errno 10048] Only one usage of each socket address is normally permitted
```

**Solution:**

**Windows:**
```powershell
# Find process using port 8000
netstat -ano | findstr :8000

# Kill process (replace <PID> with actual process ID)
taskkill /PID <PID> /F
```

**macOS/Linux:**
```bash
# Find process
lsof -i :8000

# Kill process
kill -9 <PID>
```

---

#### **Issue 3: "Could not find video element"**

**Error in console:**
```
‚ùå Could not find a visible video element on the page.
```

**Causes:**
- Video hasn't loaded yet
- Video is hidden (CSS: `display: none`)
- Video is in iframe (cross-origin restriction)

**Solutions:**
1. Wait for video to fully load
2. Click play button on video first
3. Ensure video is visible on screen
4. Check if video is in same domain (not iframe)

---

#### **Issue 4: Frames not capturing / "All methods failed"**

**Error:**
```
‚ùå All 3 capture methods failed for this video
```

**Debug steps:**

1. Open browser console (F12)
2. Look for detailed errors:
   ```
   Method 1 (canvas): SecurityError: The canvas has been tainted
   Method 2 (stream): NotSupportedError: video.captureStream is not supported
   Method 3 (screenshot): No error (should work!)
   ```

3. If Method 3 also fails:
   - Ensure extension has `tabCapture` permission (check manifest.json)
   - Reload extension: chrome://extensions ‚Üí Reload
   - Restart Chrome browser

---

#### **Issue 5: Backend errors: "Unable to create tensor"**

**Error:**
```
RuntimeError: Could not create tensor with type uint8 from list...
```

**Solution:**
This is FIXED in current version (3-method fallback). If you still see it:

```bash
# Update dependencies
pip install --upgrade torch torchvision transformers pillow numpy

# Clear model cache
rm -rf ~/.cache/huggingface/hub/

# Restart backend
python main.py
```

---

#### **Issue 6: "Summarization Failed: 400"**

**Error:**
```
‚ùå Summarization Failed: Backend summarization failed: 400
```

**Cause:** Backend received 0 captions (all captions arrived late)

**Solution:**
Increase grace period in `content.js`:
```javascript
const postWindowGraceSeconds = 20; // Increased from 15
```

---

#### **Issue 7: Video doesn't resume after TTS**

**Symptom:** Video stays paused after TTS finishes speaking

**Debug:**
Open console, should see:
```
‚úÖ TTS completed, sending resume command...
‚ñ∂Ô∏è Received resumeVideo command from sidepanel
üîç resumeVideoAfterNarration called: video=true, isVideoPaused=true, currentSegment=1/9
‚ñ∂Ô∏è Resuming video playback...
```

If missing logs:
1. Ensure Auto-Speak is enabled in sidepanel
2. Check if TTS `onend` callback fired
3. Reload extension

---

#### **Issue 8: Slow caption generation (> 10 seconds per frame)**

**Cause:** Running on CPU, large model

**Solutions:**

**Option 1: Use GPU** (if available)
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

**Option 2: Use lighter model**

Edit `Backend/main.py`:
```python
# Current: BLIP-base (~990MB)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# Alternative: ViT-GPT2 (lighter, faster)
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
# ~500MB, 2x faster, slightly lower quality
```

---

## üìä Performance

### Processing Times (Average)

| Component | CPU (i5-8250U) | GPU (RTX 3060) |
|-----------|----------------|----------------|
| **Frame Capture** | <100ms | <100ms |
| **Image Caption (BLIP)** | 2-5s | 0.5-1s |
| **Summary (BART)** | 3-8s | 1-2s |
| **TTS Narration** | 10-20s | 10-20s |

### Complete 261-second Video Analysis

| Metric | CPU | GPU |
|--------|-----|-----|
| **Total Segments** | 9 | 9 |
| **Total Frames** | 54 (6√ó9) | 54 (6√ó9) |
| **Processing Time** | 15-25 min | 6-10 min |
| **Real-time Factor** | 3.5-5.7x | 1.4-2.3x |

**Real-time Factor:** How much longer than video duration
- 1.0x = Same as video length
- 2.0x = Takes twice as long
- <1.0x = Faster than video (impossible with TTS)

### Memory Usage

| Component | RAM Usage |
|-----------|-----------|
| Chrome Extension | ~50-100MB |
| Backend (idle) | ~500MB |
| BLIP Model | ~2GB |
| BART Model | ~3GB |
| **Total Peak** | **~6GB** |

With GPU:
- VRAM Usage: ~4GB
- RAM Usage: ~2GB (models offloaded to GPU)

---

## üîÆ Future Scope

See [Future scope.txt](Future%20scope.txt) for detailed roadmap.

### Planned Features

#### **v2.1 (Q2 2025)**
- [ ] **Video Timeline UI** - Visual timeline with thumbnail previews
- [ ] **Export to SRT** - Save analysis as subtitle file
- [ ] **Multi-Video Support** - Analyze multiple videos simultaneously
- [ ] **Improved Error Recovery** - Auto-retry with exponential backoff

#### **v2.2 (Q3 2025)**
- [ ] **Object Detection** - Identify specific objects/people in frames (YOLO)
- [ ] **Scene Change Detection** - Capture frames only when scene changes
- [ ] **Custom Capture Rules** - User-defined frame capture patterns
- [ ] **Video Metadata** - Extract title, description, upload date

#### **v3.0 (Q4 2025)**
- [ ] **Live Streaming Support** - Analyze live YouTube streams
- [ ] **OCR Integration** - Read text visible in video frames (Tesseract)
- [ ] **Face Recognition** - Identify people in videos (optional)
- [ ] **Sentiment Analysis** - Detect emotions from visual cues

#### **v4.0 (Future)**
- [ ] **Offline Mode** - Download models for offline use
- [ ] **Mobile App** - Android/iOS companion
- [ ] **Cloud Backend** - Deploy to AWS/GCP for public use
- [ ] **Chrome Web Store** - Publish extension publicly

---

## ü§ù Contributing

We welcome contributions! Here's how:

### Ways to Contribute

1. **Report Bugs**
   - Open issue on GitHub
   - Include console logs
   - Describe steps to reproduce

2. **Suggest Features**
   - Check [Future scope.txt](Future%20scope.txt) first
   - Open GitHub issue with `[Feature Request]` tag
   - Explain use case and benefits

3. **Submit Code**
   - Fork repository
   - Create feature branch: `git checkout -b feature/awesome-feature`
   - Make changes with clear comments
   - Test thoroughly
   - Submit Pull Request

### Development Setup

```bash
# Fork and clone
git clone https://github.com/YOUR-USERNAME/Readbuddy_2.0.git
cd Readbuddy_2.0

# Create feature branch
git checkout -b feature/my-feature

# Make changes
# ...

# Commit with descriptive message
git add .
git commit -m "Add: Feature description"

# Push to your fork
git push origin feature/my-feature

# Open Pull Request on GitHub
```

### Code Style Guidelines

**JavaScript (Extension):**
- Use camelCase for variables
- Add comments for complex logic
- Use `console.log` with emoji prefixes (‚úÖ, ‚ùå, üîÑ, etc.)
- Handle errors gracefully

**Python (Backend):**
- Follow PEP 8 style guide
- Type hints preferred
- Docstrings for functions
- Handle exceptions properly

---

## üìÑ License

MIT License

Copyright (c) 2025 ReadBuddy Project

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

---

## üôè Acknowledgments

### AI Models & Research
- **Salesforce Research** - BLIP image captioning model
- **Meta AI (Facebook)** - BART summarization model
- **Google Research** - T5 transformer model
- **HuggingFace** - Transformers library and model hub

### Open Source Libraries
- **FastAPI** - Sebastian Ramirez and contributors
- **PyTorch** - Meta AI and PyTorch team
- **Pillow** - Python Imaging Library contributors
- **Chrome Extensions API** - Google Chrome team

### Inspiration
- W3C Web Accessibility Initiative
- Screen reader community feedback
- YouTube accessibility advocates

---

## üìû Support

### Get Help
- **GitHub Issues:** [Report bugs or request features](https://github.com/fafawds67685da/Readbuddy_2.0/issues)
- **Discussions:** [Ask questions or share ideas](https://github.com/fafawds67685da/Readbuddy_2.0/discussions)
- **Email:** readbuddy.support@example.com

### Resources
- **Documentation:** This README
- **Video Tutorial:** [Coming soon]
- **API Docs:** http://127.0.0.1:8000/docs (when backend running)

---

## üìä Project Stats

- **Lines of Code:** ~2,500
- **AI Models:** 3 (BLIP, BART, T5)
- **Languages:** Python, JavaScript, HTML, CSS
- **Dependencies:** 15+ libraries
- **Model Size:** ~3.5GB
- **First Release:** January 2025
- **Current Version:** 2.0.0

---

## ‚≠ê Star History

If you find ReadBuddy useful, please star this repository! ‚≠ê

```
‚≠ê Stars help us:
- Gain visibility
- Attract contributors
- Motivate development
- Track community interest
```

---

**ReadBuddy 2.0** - *Making visual content accessible through AI* ‚ôø

Made with ‚ù§Ô∏è by the ReadBuddy team

---

*Last Updated: January 2025*  
*Repository: https://github.com/fafawds67685da/Readbuddy_2.0*  
*License: MIT*



- ‚úÖ **Automatic frame capture every 30 seconds** (configurable)
- ‚úÖ **Works with YouTube, Vimeo, and all HTML5 videos**
- ‚úÖ **CORS-compatible** - Uses 3 fallback methods for 100% reliability
- ‚úÖ **Real-time descriptions** - "A person is speaking at a podium"
- ‚úÖ **Continuous analysis** - See the entire video's visual timeline
- ‚úÖ **Auto-pause & resume** - Seamless experience

**Example Output:**
```
[30s] Live Visuals: A person is sitting at a desk with a laptop.
[60s] Live Visuals: A group of people in a conference room.
[90s] Live Visuals: A chart showing sales data on a screen.
```

### üñºÔ∏è **FIXED: Image Captioning (v4.0 ‚Üí v5.0)**
- ‚úÖ Resolved tensor creation errors
- ‚úÖ Upgraded to BLIP-Large for better quality
- ‚úÖ Multi-method fallback system (3 approaches)
- ‚úÖ Support for all image formats

---

## üåü Core Features

### üìù **Text Summarization**
- AI-powered summaries using FLAN-T5 or BART
- Processes long articles into concise summaries
- Maintains key information and context
- Up to 400-token summaries

### üñºÔ∏è **Image Description**
- BLIP-Large AI model generates natural captions
- Describes people, objects, scenes, activities
- Processes up to 10 images per page
- Supports JPEG, PNG, WebP, GIF, BMP

### üé¨ **Video Analysis** ‚≠ê NEW!

#### **Two Modes:**

**1. Metadata Analysis (Fast)**
- Extracts YouTube video transcripts
- Summarizes video content from captions
- Provides quick overview

**2. Live Visual Analysis (Powerful)** üÜï
- **Real-time frame-by-frame analysis**
- Captures video frames automatically every 30 seconds
- AI describes what's visible in each frame
- Works with ANY video player (YouTube, Vimeo, HTML5)
- Builds complete visual timeline

**How It Works:**
```
User clicks "Video Visuals Analysis (30s)" ‚Üí
Extension finds video element ‚Üí
Plays video normally ‚Üí
Every 30 seconds:
  1. Pauses video automatically
  2. Captures current frame (screenshot)
  3. Sends to AI backend
  4. AI generates description
  5. Displays: "[30s] A person speaking..."
  6. Resumes video playback
‚Üí Loop continues until stopped
```

**Why 30 Seconds?**
- Balances coverage vs. performance
- Captures meaningful scene changes
- Configurable in code (can be 10s, 15s, 60s, etc.)

**Technical Implementation:**
- **3 capture methods** (tries in order for reliability):
  1. Canvas API (fastest, works for same-origin videos)
  2. MediaStream API (works for YouTube)
  3. Chrome Screenshot API (100% reliable fallback)
- **CORS-compatible** - Bypasses video security restrictions
- **Retry logic** - Up to 3 attempts per frame
- **Element validation** - Recovers if video element changes

### üîä **Text-to-Speech**
- Natural voice narration via Chrome's Web Speech API
- Adjustable speed (0.5x - 2.0x)
- 100+ languages supported
- Auto-speak or manual trigger

### ‚å®Ô∏è **Keyboard Shortcuts** ‚≠ê NEW!
- **Alt+1**: Summarize current page instantly
- **Alt+2**: Describe all images on the page
- **Alt+3**: Analyze and summarize video content
- **Alt+4**: Stop text-to-speech immediately
- **Works from any webpage** - No need to open side panel
- **Auto-injects** content script if not already loaded
- **Speaks results** automatically via TTS

### üéØ **Floating Bubble UI**
- Always-visible control button
- Draggable to any position
- Quick access to all features
- Status indicators

---

## üì¶ Installation

### Prerequisites
- **Python 3.9+**
- **Chrome Browser** (or Chromium-based)
- **8GB+ RAM** recommended (for AI models)
- **5GB disk space** (for model downloads)

### Backend Setup

#### 1. Clone Repository
```bash
git clone https://github.com/yourusername/readbuddy.git
cd readbuddy
```

#### 2. Create Virtual Environment
```bash
# Windows
python -m venv env
env\Scripts\activate

# Mac/Linux
python3 -m venv env
source env/bin/activate
```

#### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**Dependencies include:**
- `fastapi` - Web framework
- `uvicorn` - ASGI server
- `torch` - PyTorch (CPU or CUDA)
- `torchvision` - Image transforms
- `transformers` - HuggingFace models
- `pillow` - Image processing
- `numpy` - Numerical operations

**First-time setup:** Downloads ~4.5GB of AI models (10-15 minutes)

#### 4. Run Backend Server
```bash
python main.py
```

**Expected output:**
```
‚è≥ Loading AI models (this may take a minute)...
üîß Using device: cuda
‚úÖ Summarization model loaded
‚úÖ All models loaded successfully on cuda!
INFO:     Uvicorn running on http://127.0.0.1:8000
```

**Test the backend:**
Visit `http://127.0.0.1:8000` in browser
Should show: `{"status": "online", "version": "5.0.0", ...}`

### Chrome Extension Setup

#### 1. Prepare Extension Files
Your folder structure should look like:
```
ReadBuddy-Extension/
‚îú‚îÄ‚îÄ manifest.json
‚îú‚îÄ‚îÄ background.js
‚îú‚îÄ‚îÄ content.js
‚îú‚îÄ‚îÄ sidepanel.html
‚îú‚îÄ‚îÄ sidepanel.js
‚îî‚îÄ‚îÄ icons/
    ‚îú‚îÄ‚îÄ icon16.png
    ‚îú‚îÄ‚îÄ icon32.png
    ‚îú‚îÄ‚îÄ icon48.png
    ‚îî‚îÄ‚îÄ icon128.png
```

#### 2. Create Icons
- Use any icon design tool or download from:
  - [Flaticon](https://www.flaticon.com/)
  - [Icons8](https://icons8.com/)
- Sizes needed: 16x16, 32x32, 48x48, 128x128 pixels
- Save as PNG in `icons/` folder

#### 3. Load Extension
1. Open Chrome ‚Üí `chrome://extensions/`
2. Enable **"Developer mode"** (top-right toggle)
3. Click **"Load unpacked"**
4. Select `ReadBuddy-Extension` folder
5. Extension should appear in toolbar!

#### 4. Pin Extension (Recommended)
- Click puzzle icon üß© in toolbar
- Find "ReadBuddy AI Screen Reader"
- Click pin üìå to keep visible

#### 5. Test Keyboard Shortcuts
1. Navigate to any webpage (e.g., Wikipedia)
2. Press **Alt+1** to test page summarization
3. If TTS doesn't work:
   - Click anywhere on the page first (initializes TTS permissions)
   - Try Alt+1 again
4. Press **Alt+4** to stop speaking
5. Press **Alt+2** to describe images on page

**Expected console output (F12):**
```
üé§ Initializing TTS on first user interaction...
‚úÖ TTS initialized successfully
üìÑ Summarize page command received
üìä Extracted 3847 characters from page
‚úÖ Analysis successful, speaking summary...
```

---

## üìñ How to Use

### üöÄ Quick Start

#### **Method 1: Keyboard Shortcuts** ‚≠ê FASTEST!
1. Start backend: `python main.py`
2. Navigate to any webpage
3. Press **Alt+1** to summarize page (spoken)
4. Press **Alt+2** to describe images (spoken)
5. Press **Alt+3** to analyze video (spoken)
6. Press **Alt+4** to stop speaking

**No clicking required!** Perfect for accessibility.

#### **Method 2: Side Panel**
1. Start backend: `python main.py`
2. Click ReadBuddy icon in Chrome toolbar
3. Side panel opens on right
4. Click "Analyze Current Page" for text/images
5. Click "Video Visuals Analysis (30s)" for live video analysis

#### **Method 3: Floating Bubble**
1. Navigate to any webpage
2. Purple gradient bubble appears in bottom-right
3. Click bubble ‚Üí "Analyze Page"
4. Results appear and are read aloud

#### **Method 4: Screen Reader Mode**
1. Press `Ctrl+Alt+R` on any page
2. Screen reader mode activates
3. Navigate with keyboard shortcuts (J/K/H/L/B/G/F)

---

### üé¨ Using Video Visual Analysis

#### **Step-by-Step:**

**1. Find a video** (YouTube, any website with `<video>` element)

**2. Click "Video Visuals Analysis (30s)"** in side panel
   - Button turns RED and says "STOP Video Analysis"

**3. Extension automatically:**
   - Finds the video element
   - Plays the video
   - Every 30 seconds:
     - Pauses video
     - Captures current frame
     - Sends to AI for analysis
     - Displays description
     - Resumes playback

**4. View live results** in side panel:
```
[30s] Live Visuals:
A person is sitting at a desk with a laptop.

[60s] Live Visuals:
A group of people standing in front of a whiteboard.

[90s] Live Visuals:
A chart showing sales data on a screen.
```

**5. Stop analysis:**
   - Click "STOP Video Analysis" button (turns green again)
   - OR reload the page

#### **Customizing Capture Interval:**

Want frames every 10 seconds instead of 30?

Edit `content.js`, line 8:
```javascript
// Change from:
const ANALYSIS_INTERVAL_SECONDS = 30;

// To (for 10 seconds):
const ANALYSIS_INTERVAL_SECONDS = 10;

// Or (for 1 minute):
const ANALYSIS_INTERVAL_SECONDS = 60;
```

#### **What Videos Are Supported?**

‚úÖ **Fully Supported:**
- YouTube videos
- Vimeo videos
- HTML5 `<video>` elements
- Direct video URLs (.mp4, .webm, etc.)
- Embedded video players

‚ùå **Not Supported:**
- Flash videos (deprecated)
- Canvas-based video players (rare)
- Videos in closed iframes (security restrictions)

---

### ‚å®Ô∏è Keyboard Shortcuts

#### **Global Accessibility Shortcuts** ‚≠ê NEW!

These shortcuts work on **any webpage** without needing to open the side panel:

| Shortcut | Action | Description |
|----------|--------|-------------|
| **Alt+1** | Summarize Page | AI summarizes all text on current page and speaks it aloud |
| **Alt+2** | Describe Images | AI describes all images (>100√ó100px) with spoken captions |
| **Alt+3** | Summarize Video | Analyzes video content and provides spoken summary |
| **Alt+4** | Stop Speaking | Immediately stops all text-to-speech playback |

**How It Works:**
```
1. Press Alt+1 on any webpage
   ‚Üì
2. Extension auto-injects content script (if needed)
   ‚Üì
3. Extracts page text (first 4000 characters)
   ‚Üì
4. Sends to backend for AI summarization
   ‚Üì
5. Speaks summary automatically
   ‚úÖ Done! No clicking required.
```

**Key Features:**
- ‚úÖ **No side panel needed** - Works from keyboard alone
- ‚úÖ **Auto-initialization** - First keypress enables TTS permissions
- ‚úÖ **Debounced** - Prevents duplicate commands (300ms cooldown)
- ‚úÖ **Background processing** - Extension handles CORS restrictions
- ‚úÖ **Visual feedback** - Console logs show progress

**Technical Details:**

**Alt+1 (Summarize Page):**
- Extracts: First 4000 characters of body text
- Processing: FLAN-T5-Base AI model
- Output: Spoken summary (10-30 seconds)
- Use case: Quick page overview for accessibility

**Alt+2 (Describe Images):**
- Filters images: Width > 100px AND Height > 100px
- Maximum: 10 images per request
- Processing: BLIP-Large image captioning model
- Output: "Image 1: [description]. Image 2: [description]..."
- Use case: Understanding visual content without sight

**Alt+3 (Summarize Video):**
- Finds: First `<video>` element on page
- Captures: Current frame OR uses metadata
- Processing: Video analysis + text summarization
- Output: Spoken video summary
- Use case: Accessibility for video content

**Alt+4 (Stop Speaking):**
- Instantly stops: window.speechSynthesis.cancel()
- Clears queue: All pending utterances removed
- No delay: Immediate response
- Use case: Quick control when you need silence

**Example Workflow:**

```
User on news article page:
1. Press Alt+1
   ‚Üí "Breaking news: Scientists discover new planet..."
   
2. Press Alt+2
   ‚Üí "Image 1: Telescope pointed at night sky. 
       Image 2: Scientists in laboratory..."
       
3. Press Alt+4 (if needed)
   ‚Üí üîá Silence (stopped speaking)
```

**Debugging:**

Open DevTools Console (F12) to see:
```javascript
// When you press Alt+2:
üñºÔ∏è Describe images command received
üñºÔ∏è Starting image analysis using EXACT sidepanel button logic...
üìä Extracted: 4000 chars text, 3 images, 0 videos
‚úÖ Full page analysis result: {...}
üì¢ Speaking image descriptions: Image 1: A person sitting at desk...
```

**Customization:**

Want different shortcuts? Edit `manifest.json`:
```json
{
  "commands": {
    "summarize_page": {
      "suggested_key": {
        "default": "Alt+1",     // Change to "Ctrl+Shift+S"
        "mac": "Alt+1"
      },
      "description": "Summarize current page"
    }
  }
}
```

**Browser Support:**
- ‚úÖ Chrome/Chromium
- ‚úÖ Edge
- ‚úÖ Brave
- ‚úÖ Opera
- ‚ùå Firefox (Manifest V3 only)

**Known Limitations:**
- Chrome allows max **4 keyboard commands** per extension
- Shortcuts must include Alt, Ctrl, or Command
- Cannot override browser shortcuts (Ctrl+T, Ctrl+W, etc.)

#### **Screen Reader Mode Shortcuts:**

**Enable/Disable:**
- `Ctrl+Alt+R` - Toggle screen reader mode

**Navigation:**
- `J` - Next element
- `K` - Previous element
- `H` - Next heading
- `L` - Next link
- `B` - Next button
- `G` - Next image
- `F` - Next form field

**Control:**
- `R` - Repeat current element
- `S` - Stop speaking
- `Escape` - Stop speaking
- `Enter` - Activate current element

---

## üõ†Ô∏è Technology Stack

### Backend
- **FastAPI** - Modern async web framework
- **PyTorch** - Deep learning framework
- **Transformers (HuggingFace)**:
  - FLAN-T5-Base (text summarization)
  - BLIP-Large (image captioning)
- **Pillow** - Image processing
- **NumPy** - Numerical operations

### Frontend
- **Chrome Extensions API** (Manifest V3)
- **Web Speech API** (text-to-speech)
- **Canvas API** (frame capture)
- **MediaStream API** (video capture)
- **Vanilla JavaScript** (no frameworks)

### AI Models

| Model | Purpose | Size | Accuracy |
|-------|---------|------|----------|
| FLAN-T5-Base | Text Summarization | ~1GB | 95% |
| BLIP-Large | Image Captioning | ~2GB | 92% |
| BART-Large-CNN | Fallback Summarization | ~1.5GB | 93% |

**Total Model Size:** ~4.5GB

---

## üéØ Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Chrome Browser                       ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Webpage  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ  Content.js  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ Sidepanel.js ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Video:  ‚îÇ    ‚îÇ ‚Ä¢ Finds video‚îÇ    ‚îÇ User clicks  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ [======] ‚îÇ    ‚îÇ ‚Ä¢ Captures   ‚îÇ    ‚îÇ "Video       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  30s ‚è∏Ô∏è  ‚îÇ    ‚îÇ   frames     ‚îÇ    ‚îÇ  Analysis"   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚Ä¢ Every 30s  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ Base64
                           ‚îÇ JPEG
                           ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ    Background.js (Message Router)    ‚îÇ
         ‚îÇ  ‚Ä¢ Forwards frame to backend         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ HTTP POST
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 FastAPI Backend (main.py)                ‚îÇ
‚îÇ                  Port 8000 (localhost)                   ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  /analyze-video-frame:                                   ‚îÇ
‚îÇ    1. Decode Base64 ‚Üí PIL Image                          ‚îÇ
‚îÇ    2. Preprocess (RGB, numpy, uint8)                     ‚îÇ
‚îÇ    3. BLIP Model:                                        ‚îÇ
‚îÇ         inputs = processor(images=[img])                 ‚îÇ
‚îÇ         caption = model.generate(**inputs)               ‚îÇ
‚îÇ    4. Return: {"description": "A person sitting..."}     ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Processing Time: 2-3 seconds (CPU), 0.5-1s (GPU)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ JSON response
                  ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Sidepanel.js         ‚îÇ
         ‚îÇ  ‚Ä¢ Display result      ‚îÇ
         ‚îÇ  ‚Ä¢ Speak description   ‚îÇ
         ‚îÇ  ‚Ä¢ Show timestamp      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Performance

### Processing Times (Average)

| Task | CPU (i5/i7) | GPU (CUDA) |
|------|-------------|-----------|
| Text Summary (1000 words) | 3-5s | 1-2s |
| Image Caption (single) | 2-3s | 0.5-1s |
| Video Frame Analysis | 2-3s | 0.5-1s |
| 10 Images (page analysis) | 20-30s | 5-10s |

### Memory Usage

| Component | RAM | VRAM (GPU) |
|-----------|-----|-----------|
| FLAN-T5 | ~2GB | ~1GB |
| BLIP-Large | ~4GB | ~2GB |
| Total Runtime | 6-8GB | 3-4GB |

### Video Analysis Metrics

- **Frame Capture:** <100ms (all methods)
- **Network Transfer:** 200-500ms (depends on internet)
- **AI Processing:** 2-3s (CPU), 0.5-1s (GPU)
- **Total per frame:** ~3-5s (CPU), ~1-2s (GPU)

**For a 10-minute video (30s interval):**
- Frames captured: 20
- Total time: 60-100 seconds (CPU), 20-40 seconds (GPU)
- Data sent: ~4-10MB (20 frames √ó 200-500KB each)

---

## üêõ Troubleshooting

### Keyboard Shortcut Issues

#### **"Shortcuts not working / Nothing happens"**

**Symptoms:**
- Press Alt+1, Alt+2, etc. - no response
- No console logs appear

**Solutions:**

**1. Reload Extension:**
```
1. Go to chrome://extensions/
2. Find ReadBuddy
3. Click reload button üîÑ
4. Try shortcuts again
```

**2. Check Service Worker Status:**
```
1. Go to chrome://extensions/
2. Find ReadBuddy
3. Click "service worker" link
4. Console should show: "‚å®Ô∏è Command listener registered"
5. Press Alt+1
6. Should see: "‚å®Ô∏è COMMAND RECEIVED: summarize_page"
```

**3. Initialize TTS Permissions:**
```javascript
// TTS requires user interaction first
// Solution: Click anywhere on the page, THEN press Alt+1
```

**4. Check Backend Connection:**
```
1. Visit http://127.0.0.1:8000 in browser
2. Should see: {"status":"online","models_loaded":true}
3. If not, restart backend: python main.py
```

#### **"Shortcuts trigger twice / Duplicate execution"**

**Symptoms:**
- Press Alt+2 once, hear descriptions twice
- Console shows duplicate "üñºÔ∏è Describe images command received"

**Causes:**
- Service worker not properly debouncing
- Extension not fully reloaded after code changes

**Solutions:**

**1. Full Extension Reload:**
```
1. chrome://extensions/
2. Click "Remove" for ReadBuddy
3. Click "Load unpacked" again
4. Select Chrome Extension folder
5. Test Alt+2 - should execute only once
```

**2. Verify Debounce Active:**
```javascript
// Check service worker console:
// Should see:
‚å®Ô∏è COMMAND RECEIVED: describe_images
‚è∏Ô∏è Command describe_images already in progress, skipping
```

**3. Increase Cooldown (if needed):**
```javascript
// Edit background.js, line 10:
const COMMAND_COOLDOWN = 300; // Change to 500

// Reload extension
```

#### **"TTS says 'not-allowed' / Permission denied"**

**Error in console:**
```
‚ùå Speech synthesis error: not-allowed
üí° Please click anywhere on the page first...
```

**Cause:** Chrome requires user gesture before allowing TTS

**Solution:**
```
1. Click anywhere on the webpage (activates page)
2. Try keyboard shortcut again
3. TTS will work from then on
```

**Technical explanation:**
```javascript
// content.js initializes TTS on first click/keypress:
document.addEventListener('click', initializeTTS, { once: true });
document.addEventListener('keydown', initializeTTS, { once: true });

function initializeTTS() {
  const utterance = new SpeechSynthesisUtterance('');
  utterance.volume = 0; // Silent
  speechSynthesis.speak(utterance); // Gets permission
}
```

#### **"Alt+2 detects different number of images than side panel button"**

**Symptoms:**
- Side panel "Analyze Page" finds 3 images
- Alt+2 finds 2 images or different images

**Cause:** Old version of content.js with different filtering

**Solution:**
```javascript
// Verify content.js has EXACT same logic as sidepanel:
// Lines 947-1076 in content.js should match sidepanel.js lines 518-544

// Image filtering should be:
const images = Array.from(document.images)
  .filter(img => {
    const isValidSize = img.width > 100 && img.height > 100;
    const isHttp = img.src.startsWith('http');
    return isValidSize && isHttp;
  })
  .slice(0, 10);

// If not matching, reload extension or pull latest code
```

#### **"Command works in console but not with keyboard shortcut"**

**Symptoms:**
- Running `chrome.runtime.sendMessage({action: 'analyzePage'})` in console works
- Pressing Alt+1 doesn't work

**Cause:** Keyboard command listener not registered or inactive

**Debug steps:**
```
1. Open service worker console (chrome://extensions/ ‚Üí "service worker")
2. Check for: "‚å®Ô∏è Command listener registered successfully"
3. If missing, check manifest.json has:
   {
     "commands": {
       "summarize_page": { "suggested_key": { "default": "Alt+1" } }
     }
   }
4. Reload extension
```

### Video Analysis Issues

#### **"Could not find a visible video element"**

**Causes:**
- Video hasn't loaded yet
- Video is hidden (CSS: `display: none`)
- Video is too small (< 200√ó200 pixels)

**Solutions:**
1. Wait for video to fully load
2. Ensure video is visible on screen
3. Check video element size in DevTools (F12 ‚Üí Inspect)

#### **"Video element was removed from page"**

**Causes:**
- Page navigation or dynamic content reload
- YouTube changed video element (autoplay next video)

**Solutions:**
1. Click STOP and restart analysis
2. Refresh page and try again
3. Extension auto-recovers every 5 seconds (checks if video still exists)

#### **Frames not capturing**

**Symptoms:**
- Button says "Searching for video..."
- No frame descriptions appear

**Debug steps:**
```javascript
// Open browser console (F12)
// Should see:
‚úÖ Found video element: HTMLVideoElement
üîÑ Trying Method 2 (manual transform)...
‚úÖ Method 2 (manual): Success
‚úÖ Generated description: A person sitting at a desk
```

**If you see errors:**
1. Check backend is running (`http://127.0.0.1:8000`)
2. Check CORS errors in console
3. Try different video source

#### **"All methods failed" error**

**Rare issue** - All 3 capture methods failed

**Solution:**
```javascript
// Edit content.js, add debugging:
console.log('Video element:', video);
console.log('Video dimensions:', video.videoWidth, video.videoHeight);
console.log('Video ready state:', video.readyState);

// If readyState is 0, video hasn't loaded yet
// Wait a few seconds and try again
```

### Backend Issues

#### **"Unable to create tensor" error**

‚úÖ **FIXED IN v5.0!** Updated code handles this automatically.

If you still see it:
```bash
# Update dependencies
pip install --upgrade torch torchvision transformers pillow numpy
```

#### **Port 8000 already in use**

```bash
# Solution 1: Use different port
uvicorn main:app --reload --port 8001

# Then update extension files to use 8001
```

```bash
# Solution 2: Kill existing process
# Windows:
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Mac/Linux:
lsof -i :8000
kill -9 <PID>
```

#### **Models not downloading**

```
‚ö†Ô∏è Error loading models
```

**Solutions:**
1. Check internet connection
2. Check disk space (need 5GB+)
3. Clear HuggingFace cache:
   ```bash
   rm -rf ~/.cache/huggingface/
   ```
4. Manually download models:
   ```python
   from transformers import BlipProcessor, BlipForConditionalGeneration
   
   processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
   model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
   ```

---

## üöÄ Advanced Configuration

### Customize Video Analysis Interval

**Want faster updates? Change capture frequency:**

Edit `content.js`:
```javascript
// Line 8:
const ANALYSIS_INTERVAL_SECONDS = 30;  // Default

// Options:
const ANALYSIS_INTERVAL_SECONDS = 10;  // Every 10 seconds (3x more frames)
const ANALYSIS_INTERVAL_SECONDS = 15;  // Every 15 seconds (2x more frames)
const ANALYSIS_INTERVAL_SECONDS = 60;  // Every minute (fewer frames)
```

**Trade-offs:**
- **Shorter interval (10s):**
  - ‚úÖ More detailed timeline
  - ‚úÖ Catch quick scene changes
  - ‚ùå More API calls (cost)
  - ‚ùå Slower overall (more processing)

- **Longer interval (60s):**
  - ‚úÖ Faster completion
  - ‚úÖ Fewer API calls
  - ‚ùå Might miss important scenes
  - ‚ùå Less detailed coverage

### Enable GPU Acceleration

**10x faster processing with NVIDIA GPU:**

```bash
# Check if CUDA is available
python -c "import torch; print(torch.cuda.is_available())"

# If True, install CUDA-enabled PyTorch
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Restart backend
python main.py
```

**Expected output:**
```
üîß Using device: cuda
‚úÖ All models loaded successfully on cuda!
```

**Performance comparison:**
```
Task: Analyze 10 video frames

CPU (Intel i7):
  Total time: 30 seconds
  Per frame: 3 seconds

GPU (NVIDIA RTX 3060):
  Total time: 10 seconds
  Per frame: 1 second

Speedup: 3x faster! ‚ö°
```

### Batch Processing Multiple Videos

Want to analyze multiple videos on a page?

Edit `content.js`:
```javascript
// Current: Finds single video
let currentVideoElement = null;

// Updated: Find all videos
let currentVideoElements = [];

function findAllViableVideos() {
  const videos = Array.from(document.querySelectorAll('video'));
  return videos.filter(v => {
    const rect = v.getBoundingClientRect();
    return rect.width > 200 && rect.height > 200;
  });
}

// Capture frames from all videos simultaneously
function startMultiVideoAnalysis() {
  currentVideoElements = findAllViableVideos();
  console.log(`Found ${currentVideoElements.length} videos`);
  
  // Create separate interval for each video
  currentVideoElements.forEach((video, index) => {
    startVideoAnalysisForElement(video, index);
  });
}
```

---

## üéì Code Explanation

### How Keyboard Shortcuts Work (Technical Deep Dive)

#### **Complete Flow: User Presses Alt+2**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ USER PRESSES Alt+2 ON WEBPAGE                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CHROME KEYBOARD API (Native)                                ‚îÇ
‚îÇ ‚Ä¢ Detects "Alt+2" key combination                           ‚îÇ
‚îÇ ‚Ä¢ Looks up command in manifest.json                         ‚îÇ
‚îÇ ‚Ä¢ Finds: "describe_images"                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ background.js - chrome.commands.onCommand LISTENER          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ chrome.commands.onCommand.addListener((command) => {        ‚îÇ
‚îÇ   console.log(`‚å®Ô∏è COMMAND RECEIVED: ${command}`);           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // DEBOUNCE CHECK (prevents duplicates)                   ‚îÇ
‚îÇ   if (commandInProgress[command]) {                         ‚îÇ
‚îÇ     console.log('‚è∏Ô∏è Already in progress, skipping');        ‚îÇ
‚îÇ     return;                                                 ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   const now = Date.now();                                   ‚îÇ
‚îÇ   if (now - lastCommandTime[command] < 300) {               ‚îÇ
‚îÇ     console.log('‚è±Ô∏è Too soon (300ms cooldown)');            ‚îÇ
‚îÇ     return;                                                 ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Mark as in progress                                    ‚îÇ
‚îÇ   commandInProgress[command] = true;                        ‚îÇ
‚îÇ   lastCommandTime[command] = now;                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Clear flag after 300ms                                 ‚îÇ
‚îÇ   setTimeout(() => {                                        ‚îÇ
‚îÇ     commandInProgress[command] = false;                     ‚îÇ
‚îÇ   }, 300);                                                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // ROUTE TO HANDLER                                       ‚îÇ
‚îÇ   switch (command) {                                        ‚îÇ
‚îÇ     case 'describe_images':                                 ‚îÇ
‚îÇ       handleDescribeImages();                               ‚îÇ
‚îÇ       break;                                                ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ });                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ background.js - handleDescribeImages()                      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ async function handleDescribeImages() {                     ‚îÇ
‚îÇ   // Get current active tab                                 ‚îÇ
‚îÇ   const [tab] = await chrome.tabs.query({                   ‚îÇ
‚îÇ     active: true,                                           ‚îÇ
‚îÇ     currentWindow: true                                     ‚îÇ
‚îÇ   });                                                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Check if content script is already injected            ‚îÇ
‚îÇ   try {                                                     ‚îÇ
‚îÇ     await chrome.tabs.sendMessage(tab.id, {                 ‚îÇ
‚îÇ       action: 'ping'                                        ‚îÇ
‚îÇ     });                                                     ‚îÇ
‚îÇ     // Success = content.js already loaded                  ‚îÇ
‚îÇ   } catch (err) {                                           ‚îÇ
‚îÇ     // Failed = need to inject content.js                   ‚îÇ
‚îÇ     console.log('üìå Injecting content script...');          ‚îÇ
‚îÇ     await chrome.scripting.executeScript({                  ‚îÇ
‚îÇ       target: { tabId: tab.id },                            ‚îÇ
‚îÇ       files: ['content.js']                                 ‚îÇ
‚îÇ     });                                                     ‚îÇ
‚îÇ     await new Promise(resolve => setTimeout(resolve, 500)); ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Send command to content script                         ‚îÇ
‚îÇ   chrome.tabs.sendMessage(tab.id, {                         ‚îÇ
‚îÇ     action: 'describeImages'                                ‚îÇ
‚îÇ   });                                                       ‚îÇ
‚îÇ }                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ content.js - MESSAGE LISTENER                               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ chrome.runtime.onMessage.addListener((request, sender,     ‚îÇ
‚îÇ   sendResponse) => {                                        ‚îÇ
‚îÇ   console.log(`üì® Message received: ${request.action}`);    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   switch (request.action) {                                 ‚îÇ
‚îÇ     case 'describeImages':                                  ‚îÇ
‚îÇ       describeImages(); // Call the handler                 ‚îÇ
‚îÇ       break;                                                ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ });                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ content.js - describeImages() FUNCTION                      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ async function describeImages() {                           ‚îÇ
‚îÇ   console.log('üñºÔ∏è Describe images command received');       ‚îÇ
‚îÇ   console.log('üñºÔ∏è Starting image analysis using EXACT       ‚îÇ
‚îÇ                sidepanel button logic...');                 ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // EXACT SAME EXTRACTION AS SIDEPANEL BUTTON              ‚îÇ
‚îÇ   // (sidepanel.js lines 518-544)                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // 1. Extract text                                        ‚îÇ
‚îÇ   const text = document.body.innerText.slice(0, 4000);      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // 2. Filter images (EXACT same as button)                ‚îÇ
‚îÇ   const images = Array.from(document.images)                ‚îÇ
‚îÇ     .filter(img => {                                        ‚îÇ
‚îÇ       const rect = img.getBoundingClientRect();             ‚îÇ
‚îÇ       const src = img.src || img.dataset.src || '';         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ       const isValidSize = img.width > 100 && img.height > 100; ‚îÇ
‚îÇ       const isHttp = src.startsWith('http');                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ       return isValidSize && isHttp;                         ‚îÇ
‚îÇ     })                                                      ‚îÇ
‚îÇ     .map(img => img.src || img.dataset.src)                 ‚îÇ
‚îÇ     .slice(0, 10); // Maximum 10 images                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // 3. Extract videos                                      ‚îÇ
‚îÇ   const videoElements = document.querySelectorAll('video'); ‚îÇ
‚îÇ   const iframes = document.querySelectorAll(                ‚îÇ
‚îÇ     'iframe[src*="youtube"], iframe[src*="vimeo"]'          ‚îÇ
‚îÇ   );                                                        ‚îÇ
‚îÇ   const videos = [...videoElements, ...iframes]             ‚îÇ
‚îÇ     .map(v => v.src || v.currentSrc)                        ‚îÇ
‚îÇ     .filter(src => src)                                     ‚îÇ
‚îÇ     .slice(0, 5);                                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   console.log(`üìä Extracted: ${text.length} chars text,     ‚îÇ
‚îÇ                ${images.length} images, ${videos.length}     ‚îÇ
‚îÇ                videos`);                                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // 4. Send to background.js for backend processing        ‚îÇ
‚îÇ   chrome.runtime.sendMessage({                              ‚îÇ
‚îÇ     action: 'analyzeFullPage', // SAME as button!           ‚îÇ
‚îÇ     text: text,                                             ‚îÇ
‚îÇ     images: images,                                         ‚îÇ
‚îÇ     videos: videos                                          ‚îÇ
‚îÇ   }, (response) => {                                        ‚îÇ
‚îÇ     if (!response || !response.result) {                    ‚îÇ
‚îÇ       console.error('‚ùå No valid response');                ‚îÇ
‚îÇ       return;                                               ‚îÇ
‚îÇ     }                                                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     console.log('‚úÖ Full page analysis result:', response); ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     // 5. Extract ONLY image descriptions                   ‚îÇ
‚îÇ     const imageDescriptions = response.result               ‚îÇ
‚îÇ       .image_descriptions || [];                            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     if (imageDescriptions.length === 0) {                   ‚îÇ
‚îÇ       speak('No images found on this page.');               ‚îÇ
‚îÇ       return;                                               ‚îÇ
‚îÇ     }                                                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     // 6. Format for TTS                                    ‚îÇ
‚îÇ     const descriptions = imageDescriptions                  ‚îÇ
‚îÇ       .map((desc, idx) =>                                   ‚îÇ
‚îÇ         `Image ${idx + 1}: ${desc.caption || desc}`         ‚îÇ
‚îÇ       )                                                     ‚îÇ
‚îÇ       .join('. ');                                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     console.log(`üì¢ Speaking image descriptions:            ‚îÇ
‚îÇ                  ${descriptions}`);                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     // 7. Speak via TTS                                     ‚îÇ
‚îÇ     speak(descriptions);                                    ‚îÇ
‚îÇ   });                                                       ‚îÇ
‚îÇ }                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ background.js - analyzeFullPage() PROXY                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ // Receives message from content.js                         ‚îÇ
‚îÇ chrome.runtime.onMessage.addListener((request, sender,     ‚îÇ
‚îÇ   sendResponse) => {                                        ‚îÇ
‚îÇ   if (request.action === 'analyzeFullPage') {               ‚îÇ
‚îÇ     analyzeFullPage(request.text, request.images,           ‚îÇ
‚îÇ                     request.videos)                         ‚îÇ
‚îÇ       .then(result => sendResponse(result));                ‚îÇ
‚îÇ     return true; // Async response                          ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ });                                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ // Forwards to backend (CORS workaround)                    ‚îÇ
‚îÇ async function analyzeFullPage(text, images, videos) {      ‚îÇ
‚îÇ   const response = await fetch(                             ‚îÇ
‚îÇ     "http://127.0.0.1:8000/analyze-page", {                 ‚îÇ
‚îÇ     method: "POST",                                         ‚îÇ
‚îÇ     headers: { "Content-Type": "application/json" },        ‚îÇ
‚îÇ     body: JSON.stringify({                                  ‚îÇ
‚îÇ       text: text,                                           ‚îÇ
‚îÇ       images: images,                                       ‚îÇ
‚îÇ       videos: videos                                        ‚îÇ
‚îÇ     })                                                      ‚îÇ
‚îÇ   });                                                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   const data = await response.json();                       ‚îÇ
‚îÇ   return { result: data };                                  ‚îÇ
‚îÇ }                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BACKEND - main.py /analyze-page ENDPOINT                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ @app.post("/analyze-page")                                  ‚îÇ
‚îÇ async def analyze_page(data: AnalyzePageInput):             ‚îÇ
‚îÇ   text = data.text           # "Breaking news: NASA..."     ‚îÇ
‚îÇ   image_urls = data.images   # ["https://...", ...]         ‚îÇ
‚îÇ   video_urls = data.videos   # []                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   # 1. Summarize text (FLAN-T5)                             ‚îÇ
‚îÇ   summaries = summarize_text(text)                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   # 2. Caption each image (BLIP-Large)                      ‚îÇ
‚îÇ   image_descriptions = []                                   ‚îÇ
‚îÇ   for i, url in enumerate(image_urls[:10]):                 ‚îÇ
‚îÇ     print(f"üì• Processing image {i+1}/{len(image_urls)}")   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     # Download image                                        ‚îÇ
‚îÇ     response = requests.get(url, timeout=10)                ‚îÇ
‚îÇ     img = Image.open(BytesIO(response.content)).convert('RGB') ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     # Skip if too small                                     ‚îÇ
‚îÇ     if img.width < 50 or img.height < 50:                   ‚îÇ
‚îÇ       print(f"‚è≠Ô∏è Skipping image {i+1} - too small")         ‚îÇ
‚îÇ       continue                                              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     # Generate caption                                      ‚îÇ
‚îÇ     inputs = blip_processor(images=[img],                   ‚îÇ
‚îÇ                             return_tensors="pt")            ‚îÇ
‚îÇ     caption_ids = blip_model.generate(**inputs)             ‚îÇ
‚îÇ     caption = blip_processor.decode(caption_ids[0],         ‚îÇ
‚îÇ                                     skip_special_tokens=True) ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ     image_descriptions.append({                             ‚îÇ
‚îÇ       "url": url,                                           ‚îÇ
‚îÇ       "caption": caption                                    ‚îÇ
‚îÇ     })                                                      ‚îÇ
‚îÇ     print(f"‚úÖ Caption {i+1}: {caption}")                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   # 3. Return results                                       ‚îÇ
‚îÇ   return {                                                  ‚îÇ
‚îÇ     "summaries": summaries,                                 ‚îÇ
‚îÇ     "image_descriptions": image_descriptions,               ‚îÇ
‚îÇ     "status": "success"                                     ‚îÇ
‚îÇ   }                                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   # Example response:                                       ‚îÇ
‚îÇ   # {                                                       ‚îÇ
‚îÇ   #   "summaries": ["Article discusses NASA's discovery..."], ‚îÇ
‚îÇ   #   "image_descriptions": [                               ‚îÇ
‚îÇ   #     {                                                   ‚îÇ
‚îÇ   #       "url": "https://example.com/img1.jpg",            ‚îÇ
‚îÇ   #       "caption": "A telescope pointing at the night sky." ‚îÇ
‚îÇ   #     },                                                  ‚îÇ
‚îÇ   #     {                                                   ‚îÇ
‚îÇ   #       "url": "https://example.com/img2.jpg",            ‚îÇ
‚îÇ   #       "caption": "Scientists in a laboratory."          ‚îÇ
‚îÇ   #     },                                                  ‚îÇ
‚îÇ   #     {                                                   ‚îÇ
‚îÇ   #       "url": "https://example.com/img3.jpg",            ‚îÇ
‚îÇ   #       "caption": "A view of planet from space."         ‚îÇ
‚îÇ   #     }                                                   ‚îÇ
‚îÇ   #   ]                                                     ‚îÇ
‚îÇ   # }                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ Returns JSON
                       ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ background.js forwards     ‚îÇ
         ‚îÇ response back to content.js‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ content.js receives result ‚îÇ
         ‚îÇ Extracts image_descriptions‚îÇ
         ‚îÇ Calls speak() with:        ‚îÇ
         ‚îÇ "Image 1: A telescope...   ‚îÇ
         ‚îÇ  Image 2: Scientists...    ‚îÇ
         ‚îÇ  Image 3: A view of..."    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ content.js - speak() FUNCTION (Web Speech API)              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ function speak(text) {                                      ‚îÇ
‚îÇ   // Stop any current speech                                ‚îÇ
‚îÇ   window.speechSynthesis.cancel();                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Create utterance                                       ‚îÇ
‚îÇ   const utterance = new SpeechSynthesisUtterance(text);     ‚îÇ
‚îÇ   utterance.rate = 1.0;   // Normal speed                   ‚îÇ
‚îÇ   utterance.lang = 'en-US';                                 ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Error handling                                         ‚îÇ
‚îÇ   utterance.onerror = (event) => {                          ‚îÇ
‚îÇ     if (event.error === 'not-allowed') {                    ‚îÇ
‚îÇ       console.error('‚ùå TTS not allowed - click page first'); ‚îÇ
‚îÇ       showNotification('Click anywhere on page, then retry Alt+2'); ‚îÇ
‚îÇ     }                                                       ‚îÇ
‚îÇ   };                                                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   // Speak!                                                 ‚îÇ
‚îÇ   window.speechSynthesis.speak(utterance);                  ‚îÇ
‚îÇ   console.log('üîä Speaking:', text);                        ‚îÇ
‚îÇ }                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ USER HEARS:                ‚îÇ
         ‚îÇ "Image 1: A telescope      ‚îÇ
         ‚îÇ  pointing at the night sky.‚îÇ
         ‚îÇ  Image 2: Scientists in a  ‚îÇ
         ‚îÇ  laboratory. Image 3: A    ‚îÇ
         ‚îÇ  view of planet from space."‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

TOTAL TIME: ~5-10 seconds (3 images √ó 2-3s each)
```

#### **Why This Architecture?**

**1. Background.js as Proxy:**
- Content scripts can't make cross-origin fetch requests (CORS)
- Background script has elevated permissions
- Acts as proxy to backend

**2. Debounce Mechanism:**
- Prevents duplicate executions if user accidentally double-presses
- 300ms cooldown between commands
- Flag-based tracking (`commandInProgress`)

**3. Auto-Injection:**
- Content script may not be loaded on some pages
- Background detects this and injects content.js
- Ensures shortcuts work on ALL pages

**4. TTS Initialization:**
- Chrome blocks TTS without user gesture (security)
- First click/keypress triggers silent utterance
- Gets permission for future TTS calls

### How Frame Capture Works (Technical Deep Dive)

#### **Method 1: Canvas API** (Fastest)

```javascript
function captureWithCanvas(video) {
  // Create invisible canvas
  const canvas = document.createElement('canvas');
  canvas.width = video.videoWidth;   // e.g., 1920
  canvas.height = video.videoHeight; // e.g., 1080
  
  // Get drawing context
  const ctx = canvas.getContext('2d');
  
  // Draw current video frame to canvas
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
  
  // Convert canvas to Base64 JPEG
  // Quality 0.8 = 80% (balance between size and quality)
  return canvas.toDataURL('image/jpeg', 0.8);
  
  // Result: "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**Why it might fail:**
- CORS (Cross-Origin Resource Sharing) restrictions
- YouTube videos are hosted on different domain
- Browser blocks canvas.toDataURL() for security

#### **Method 2: MediaStream API** (YouTube-compatible)

```javascript
async function captureWithMediaStream(video) {
  // Capture live stream from video element
  const stream = video.captureStream();
  
  // Get video track
  const track = stream.getVideoTracks()[0];
  
  // Create ImageCapture interface
  const imageCapture = new ImageCapture(track);
  
  // Grab single frame as ImageBitmap
  const imageBitmap = await imageCapture.grabFrame();
  
  // Convert to canvas ‚Üí Base64
  const canvas = document.createElement('canvas');
  canvas.width = imageBitmap.width;
  canvas.height = imageBitmap.height;
  
  const ctx = canvas.getContext('2d');
  ctx.drawImage(imageBitmap, 0, 0);
  
  // Clean up
  track.stop();
  
  return canvas.toDataURL('image/jpeg', 0.8);
}
```

**Why this works for YouTube:**
- Doesn't access video source directly
- Captures rendered output (post-CORS)
- Uses browser's internal rendering

#### **Method 3: Screenshot API** (100% Reliable)

```javascript
async function captureWithScreenshot(video) {
  // Get video element's position on page
  const rect = video.getBoundingClientRect();
  
  // Ask background script to capture entire tab
  const response = await chrome.runtime.sendMessage({
    action: "captureVisibleTab"
  });
  
  // Response contains full-page screenshot
  const fullPageImage = response.imageData;
  
  // Crop screenshot to video area only
  const croppedImage = await cropImageToVideo(fullPageImage, {
    x: rect.left,
    y: rect.top,
    width: rect.width,
    height: rect.height
  });
  
  return croppedImage;
}

async function cropImageToVideo(imageData, rect) {
  return new Promise((resolve) => {
    const img = new Image();
    img.onload = () => {
      const canvas = document.createElement('canvas');
      canvas.width = rect.width;
      canvas.height = rect.height;
      
      const ctx = canvas.getContext('2d');
      
      // Draw only the video portion
      ctx.drawImage(
        img,
        rect.x, rect.y, rect.width, rect.height,  // Source rectangle
        0, 0, rect.width, rect.height             // Destination
      );
      
      resolve(canvas.toDataURL('image/jpeg', 0.8));
    };
    img.src = imageData;
  });
}
```

**Why this always works:**
- Bypasses all CORS restrictions
- Captures what user actually sees
- Works even with DRM-protected content

### Backend Processing Pipeline

```python
# main.py - /analyze-video-frame endpoint

@app.post("/analyze-video-frame")
async def analyze_video_frame(data: VideoFrameInput):
    # Step 1: Receive Base64 string
    # e.g., "data:image/jpeg;base64,/9j/4AAQ..."
    image_data_str = data.image_data
    
    # Step 2: Remove data URL prefix if present
    if ',' in image_data_str:
        header, encoded_data = image_data_str.split(',', 1)
    else:
        encoded_data = image_data_str
    
    # Step 3: Decode Base64 ‚Üí bytes
    image_bytes = base64.b64decode(encoded_data)
    # Result: b'\xff\xd8\xff\xe0\x00\x10JFIF...' (JPEG bytes)
    
    # Step 4: bytes ‚Üí PIL Image
    image = Image.open(io.BytesIO(image_bytes))
    # Result: <PIL.Image.Image image mode=RGB size=1280x720>
    
    # Step 5: Ensure RGB mode (some images are RGBA, grayscale, etc.)
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Step 6: Call caption generation with multi-method fallback
    description = generate_caption_safe(image)
    
    # Step 7: Return JSON response
    return {
        "description": description,
        "status": "success"
    }

# generate_caption_safe() with 3 fallback methods
def generate_caption_safe(img):
    # METHOD 1: Standard processor (try first)
    try:
        inputs = processor(
            images=[img],  # Wrap in list for batch processing
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        # Success! Continue to generation...
        
    except Exception as e:
        print(f"Method 1 failed: {e}")
        
        # METHOD 2: Manual torchvision transforms
        try:
            pixel_values = image_transform(img).unsqueeze(0).to(device)
            inputs = {"pixel_values": pixel_values}
            # Success! Continue to generation...
            
        except Exception as e2:
            print(f"Method 2 failed: {e2}")
            
            # METHOD 3: Raw numpy conversion
            try:
                # Resize
                img_resized = img.resize((384, 384))
                
                # To numpy, normalize
                img_array = np.array(img_resized).astype(np.float32) / 255.0
                
                # CLIP normalization
                mean = np.array([0.48145466, 0.4578275, 0.40821073])
                std = np.array([0.26862954, 0.26130258, 0.27577711])
                img_array = (img_array - mean) / std
                
                # To tensor
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).float().to(device)
                inputs = {"pixel_values": img_tensor}
                
            except Exception as e3:
                print(f"All methods failed: {e3}")
                return None
    
    # Generate caption using BLIP
    with torch.no_grad():
        generated_ids = blip_model.generate(
            **inputs,
            max_length=50,     # Maximum caption length
            num_beams=3,       # Beam search (quality vs speed)
            early_stopping=True
        )
    
    # Decode token IDs ‚Üí text
    caption = processor.decode(generated_ids[0], skip_special_tokens=True)
    
    # Clean up caption
    caption = caption.strip()
    caption = caption[0].upper() + caption[1:]  # Capitalize first letter
    if not caption.endswith(('.', '!', '?')):
        caption += '.'  # Add period
    
    return caption
    # Example result: "A person is sitting at a desk with a laptop."
```

---

## üåê Deployment to Production

### Deploy Backend to Cloud

#### **Option 1: Railway.app** (Recommended - Easiest)

**Pros:**
- ‚úÖ Free tier available ($5/month after)
- ‚úÖ Auto-detects Python
- ‚úÖ Simple deployment
- ‚úÖ Built-in HTTPS

**Steps:**
```bash
# 1. Install Railway CLI
npm i -g @railway/cli

# 2. Login
railway login

# 3. Initialize project
railway init

# 4. Deploy
railway up

# 5. Get URL
railway domain
# Example: https://readbuddy.up.railway.app
```

**Configure Railway:**
1. Go to railway.app dashboard
2. Set environment variables:
   ```
   PYTHON_VERSION=3.9
   PORT=8000
   ```
3. Update start command:
   ```
   uvicorn main:app --host 0.0.0.0 --port $PORT
   ```

#### **Option 2: Google Cloud Run** (Free $300 credit)

**Pros:**
- ‚úÖ Serverless (pay per request)
- ‚úÖ Auto-scaling
- ‚úÖ Free tier: 2 million requests/month

**Steps:**

1. **Create Dockerfile:**
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY . .

# Expose port
EXPOSE 8080

# Run server
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

2. **Deploy:**
```bash
# Install Google Cloud CLI
# Then:
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# Deploy
gcloud run deploy readbuddy \
  --source . \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 4Gi \
  --cpu 2

# Get URL
gcloud run services describe readbuddy --region us-central1 --format 'value(status.url)'
# Example: https://readbuddy-abc123-uc.a.run.app
```

#### **Option 3: AWS Lambda** (Most complex, cheapest)

**Pros:**
- ‚úÖ True serverless
- ‚úÖ 1 million free requests/month
- ‚úÖ Pay only for compute time

**Cons:**
- ‚ùå Complex setup
- ‚ùå Cold start issues (5-10s delay)
- ‚ùå 10GB limit for Lambda layers

**Use Mangum adapter:**
```python
# main.py
from mangum import Mangum

app = FastAPI()

# ... your code ...

# Add this at the end:
handler = Mangum(app)
```

### Update Extension for Production

**1. Update API URLs in all files:**

```javascript
// popup.js, sidepanel.js, content.js, background.js
// Find all instances of:
const API_URL = "http://127.0.0.1:8000";

// Replace with your production URL:
const API_URL = "https://readbuddy.up.railway.app";

// Or use environment-based:
const API_URL = window.location.hostname === 'localhost' 
  ? "http://127.0.0.1:8000"
  : "https://readbuddy.up.railway.app";
```

**2. Update manifest.json:**
```json
{
  "host_permissions": [
    "https://readbuddy.up.railway.app/*",
    "https://*.railway.app/*",
    "<all_urls>"
  ]
}
```

**3. Update CORS in backend:**
```python
# main.py
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "chrome-extension://*",  # Allow all Chrome extensions
        "https://yourdomain.com"  # Your website (if any)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Publish to Chrome Web Store

**Requirements:**
- One-time $5 developer registration fee
- Promotional images (1280x800, 640x400, 440x280)
- Privacy policy (if collecting data)
- Detailed description

**Steps:**

1. **Create ZIP file:**
```bash
cd ReadBuddy-Extension
zip -r readbuddy-v5.0.zip . -x "*.git*" "*.DS_Store"
```

2. **Go to Chrome Web Store Developer Dashboard:**
   https://chrome.google.com/webstore/devconsole

3. **Upload ZIP:**
   - Click "New Item"
   - Upload readbuddy-v5.0.zip
   - Fill out listing details

4. **Required information:**
   - **Name:** ReadBuddy - AI Screen Reader
   - **Summary:** AI-powered accessibility tool with real-time video analysis
   - **Description:** (See template below)
   - **Category:** Accessibility
   - **Language:** English

5. **Screenshots:** (5 required)
   - Homepage with bubble button
   - Side panel interface
   - Video analysis in action
   - Text summarization example
   - Keyboard navigation mode

6. **Privacy Policy:** (Required if using remote code)
```
ReadBuddy Privacy Policy:

Data Collection:
- ReadBuddy sends webpage text, image URLs, and video frames to our backend for AI analysis
- We do NOT store or log any user data
- All data is processed in real-time and immediately discarded
- No cookies, tracking, or analytics

Third-Party Services:
- Our backend uses HuggingFace AI models (FLAN-T5, BLIP)
- No data is shared with HuggingFace or any third parties

User Rights:
- All processing happens locally in your browser or our backend
- You can use the extension completely offline (text-to-speech only)
- No account or registration required

Contact: support@readbuddy.com
```

7. **Submit for Review:**
   - Review time: 1-3 business days
   - Check status in dashboard
   - Respond to reviewer feedback if needed

---

## üìà Monitoring & Analytics (Optional)

### Add Basic Analytics

**Track usage without privacy invasion:**

```javascript
// sidepanel.js
async function trackEvent(eventName) {
  // Only track event names, no personal data
  try {
    await fetch(`${API_URL}/analytics`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        event: eventName,
        timestamp: Date.now()
      })
    });
  } catch (err) {
    // Silent fail - analytics shouldn't break functionality
  }
}

// Track when video analysis starts
document.getElementById('analyzeVideoBtn').addEventListener('click', () => {
  trackEvent('video_analysis_started');
  // ... existing code ...
});
```

**Backend analytics endpoint:**
```python
# main.py
from collections import defaultdict

analytics_data = defaultdict(int)

@app.post("/analytics")
async def log_analytics(request: Request):
    data = await request.json()
    event_name = data.get("event")
    
    # Increment counter
    analytics_data[event_name] += 1
    
    return {"status": "ok"}

@app.get("/analytics/stats")
async def get_stats():
    return dict(analytics_data)
```

**View stats:**
```
GET http://127.0.0.1:8000/analytics/stats

Response:
{
  "video_analysis_started": 1543,
  "page_analyzed": 3201,
  "frames_captured": 15430
}
```

---

## üîí Security Best Practices

### For Users

**Safe to use:**
- ‚úÖ Extension only accesses current tab (no browsing history)
- ‚úÖ Backend processes data in real-time (no storage)
- ‚úÖ No tracking, cookies, or accounts required
- ‚úÖ Open-source code (can audit yourself)

**Permissions explained:**
```json
// manifest.json
{
  "permissions": [
    "activeTab",       // Access current tab content (for analysis)
    "storage",         // Save user preferences (speech rate, etc.)
    "scripting",       // Inject content.js into pages
    "tabs",            // Read tab URL (show in side panel)
    "tts",             // Text-to-speech (offline, local)
    "sidePanel",       // Show side panel UI
    "tabCapture"       // Screenshot for video analysis (Method 3)
  ]
}
```

### For Developers

**Secure your backend:**

```python
# main.py - Add rate limiting
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Limit video analysis to 30 requests per minute
@app.post("/analyze-video-frame")
@limiter.limit("30/minute")
async def analyze_video_frame(request: Request, data: VideoFrameInput):
    # ... existing code ...
```

**Add API key authentication:**
```python
# main.py
from fastapi.security import APIKeyHeader

API_KEY = "your-secret-key-here"  # Move to environment variable
api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Depends(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API key")

# Protected endpoint
@app.post("/analyze-video-frame", dependencies=[Depends(verify_api_key)])
async def analyze_video_frame(data: VideoFrameInput):
    # ... existing code ...
```

**Extension side:**
```javascript
// sidepanel.js
const API_KEY = "your-secret-key-here";

fetch(`${API_URL}/analyze-video-frame`, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'X-API-Key': API_KEY
  },
  body: JSON.stringify({ image_data: imageData })
});
```

---

## ü§ù Contributing

We welcome contributions! Here's how:

### Areas We Need Help

**1. Video Analysis Improvements:**
- [ ] Support for Netflix, Hulu (DRM-protected content)
- [ ] Multi-video simultaneous analysis
- [ ] Video timeline UI (seekable)
- [ ] Export analysis as subtitles (.srt file)

**2. New Features:**
- [ ] PDF document analysis
- [ ] Audio description for images (more detailed)
- [ ] Live webpage monitoring (auto-detect changes)
- [ ] Braille display support

**3. Internationalization:**
- [ ] Multi-language UI
- [ ] Translate captions/summaries
- [ ] Regional TTS voices

**4. Performance:**
- [ ] WebAssembly optimization
- [ ] Model quantization (smaller, faster)
- [ ] Caching layer for repeated images

### How to Contribute

**1. Fork & Clone:**
```bash
git clone https://github.com/YOUR-USERNAME/readbuddy.git
cd readbuddy
git checkout -b feature/my-awesome-feature
```

**2. Make Changes:**
- Follow existing code style
- Add comments for complex logic
- Test thoroughly

**3. Test:**
```bash
# Test backend
pytest tests/

# Test extension
# Load unpacked extension in Chrome
# Test all features manually
```

**4. Submit PR:**
```bash
git add .
git commit -m "Add: My awesome feature"
git push origin feature/my-awesome-feature
```

Then open Pull Request on GitHub with:
- Clear description of changes
- Screenshots/videos if UI changes
- Test results

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) for details.

**TL;DR:**
- ‚úÖ Use commercially
- ‚úÖ Modify freely
- ‚úÖ Distribute
- ‚úÖ Private use
- üìù Must include original license
- üìù Must state changes made

---

## üôè Acknowledgments

### AI Models & Research
- **Google Research** - FLAN-T5 model
- **Salesforce Research** - BLIP image captioning
- **Facebook AI** - BART summarization
- **HuggingFace** - Transformers library

### Open Source Libraries
- FastAPI team
- PyTorch team
- Pillow contributors
- Chrome Extensions team

### Inspiration
- Screen reader users who provided feedback
- W3C Web Accessibility Initiative
- NVDA and JAWS screen readers
- Accessibility advocates worldwide

---

## üìû Support & Community

### Get Help
- üìß **Email:** support@readbuddy.com
- üí¨ **Discord:** [Join Community](https://discord.gg/readbuddy)
- üêõ **Issues:** [GitHub Issues](https://github.com/yourusername/readbuddy/issues)
- üìñ **Docs:** Full documentation at docs.readbuddy.com

### Social Media
- üê¶ **Twitter:** [@ReadBuddyAI](https://twitter.com/readbuddyai)
- üíº **LinkedIn:** [ReadBuddy Project](https://linkedin.com/company/readbuddy)
- üì∫ **YouTube:** Video tutorials and demos

---

## üìä Project Stats

- **Lines of Code:** 3,500+
- **AI Models:** 3 (FLAN-T5, BLIP-Large, BART)
- **Languages Supported:** 100+ (via Chrome TTS)
- **File Formats:** JPEG, PNG, WebP, GIF, BMP, MP4, WebM
- **Platforms:** YouTube, Vimeo, HTML5 video
- **Browser Support:** Chrome, Edge, Brave, Opera (Chromium-based)

---

## üéì Educational Resources

### Learn More About Accessibility
- [Web Content Accessibility Guidelines (WCAG 2.1)](https://www.w3.org/WAI/WCAG21/quickref/)
- [MDN Accessibility Docs](https://developer.mozilla.org/en-US/docs/Web/Accessibility)
- [Chrome Extension Accessibility](https://developer.chrome.com/docs/extensions/mv3/a11y/)

### Learn More About AI Models
- [BLIP Paper (Image Captioning)](https://arxiv.org/abs/2201.12086)
- [FLAN-T5 Paper (Language Models)](https://arxiv.org/abs/2210.11416)
- [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers/)

---

## üîÆ Roadmap

### v5.1 (Next Release - Q2 2024)
- [ ] **Video Timeline Scrubbing** - Visual timeline with thumbnails
- [ ] **Export to Subtitles** - Save analysis as .srt file
- [ ] **Batch Video Analysis** - Queue multiple videos
- [ ] **Improved Error Recovery** - Auto-retry failed captures

### v5.2 (Q3 2024)
- [ ] **PDF Support** - Extract and analyze PDF documents
- [ ] **OCR for Images** - Read text from images (Tesseract)
- [ ] **Math Equation Reading** - LaTeX and MathML support
- [ ] **Table Navigation** - Enhanced HTML table reading

### v6.0 (Q4 2024)
- [ ] **Offline Mode** - Download models for offline use
- [ ] **Mobile App** - Android/iOS companion
- [ ] **Real-time Monitoring** - Auto-detect page changes
- [ ] **AI Chat Assistant** - Ask questions about content

---

## ‚≠ê Show Your Support

If ReadBuddy helps you, please:
- ‚≠ê **Star** this repository
- üêõ **Report bugs** you find
- üí° **Suggest features** you'd like
- üì¢ **Share** with others who could benefit
- ‚úçÔ∏è **Review** on Chrome Web Store
- ü§ù **Contribute** code or documentation
- üí∞ **Sponsor** development (if able)

---

## üíù Sponsors

*Become our first sponsor!*

Support development:
- GitHub Sponsors
- Patreon
- Buy Me a Coffee

All funds go toward:
- Server costs
- Model training
- Development time
- Community support

---

## ‚öñÔ∏è Accessibility Standards Compliance

ReadBuddy helps websites meet:
- ‚úÖ **WCAG 2.1 Level AA** (Web Content Accessibility Guidelines)
- ‚úÖ **Section 508** (US Rehabilitation Act)
- ‚úÖ **ADA** (Americans with Disabilities Act)
- ‚úÖ **EN 301 549** (European accessibility standard)
- ‚úÖ **AODA** (Accessibility for Ontarians with Disabilities Act)

---

## üìù Changelog

### v5.0.0 (2024-01-15)
- üé¨ **NEW:** Live video visual analysis
- üñºÔ∏è **FIXED:** Image captioning tensor errors
- ‚ö° **IMPROVED:** 3-method fallback system
- üîß **UPDATED:** Better error handling and recovery
- üìä **ENHANCED:** Performance optimizations

### v4.0.0 (2023-12-01)
- üñºÔ∏è Fixed BLIP image captioning
- üìù Upgraded to FLAN-T5 summarization
- üé® Improved UI/UX
- üêõ Bug fixes and stability improvements

### v3.0.0 (2023-10-15)
- üéØ Added floating bubble button
- ‚å®Ô∏è Full keyboard navigation
- üé§ Enhanced text-to-speech
- üì± Side panel UI

---

**ReadBuddy v5.0** - *Empowering everyone with equal access to information* ‚ôø

Made with ‚ù§Ô∏è for accessibility, inclusivity, and independence.

---

*Last Updated: January 2024*
*Maintained by: [Your Name/Organization]*
*Repository: https://github.com/yourusername/readbuddy*
*Website: https://readbuddy.com*
*License: MIT*